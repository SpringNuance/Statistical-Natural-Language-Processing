{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fc4b522161dc493f6e43bf309b7b7be",
     "grade": false,
     "grade_id": "cell-e823893319cc2093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 7: Machine Translation Evaluation\n",
    "\n",
    "# Released: 05.03.2024\n",
    "# Deadline: 15.03.2024 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871c0bff5de00c3a7fb59ac04c9059ab",
     "grade": false,
     "grade_id": "cell-2b4616209ec110f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this assignment, you will learn how machine translation systems can be automatically evaluated, using the BLEU score.\n",
    "\n",
    "KEYWORDS:\n",
    "* BLEU\n",
    "\n",
    "Do not use external libraries in this assignment. Python standard library is allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d058bcc1b883e9aee84f50f123571ab",
     "grade": false,
     "grade_id": "cell-e1d385c1c09cab25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [TASK 1: BLEU score](#task1)\n",
    "    * [Step 1.1: Match N-grams](#subtask1_1)\n",
    "    * [Step 1.2: Match lengths](#subtask1_2)\n",
    "    * [Step 1.3: Put it together](#subtask1_3)\n",
    "    * [Step 1.4: Reflect on BLEU](#subtask1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "604663eb04e8bfb8cbe844d42fde780b",
     "grade": false,
     "grade_id": "cell-4f67e9fc3d6c389a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluation is a particularly difficult topic in machine translation. Human evaluations are preferred, but expensive. There are multiple automatic evaluation metrics in use. The most common one is BLEU. In this assignment you will implement BLEU calculation - it is computationally very simple.\n",
    "\n",
    "## TASK 1 <a class=\"anchor\" id=\"task1\"></a>\n",
    "## BLEU score\n",
    "\n",
    "**BLEU**, or bilingual evaluation understudy, was proposed in 2002, here: https://www.aclweb.org/anthology/P02-1040.pdf. Take read through the paper, it is clearly written and provides not just the algorithm, but also context and reasons for the design choices. We will implement the algorithm as described by the paper, so you can refer to it.\n",
    "\n",
    "### High-level overview\n",
    "\n",
    "The essential requirement for BLEU is an annotated test corpus. The test corpus consists of source language segments and multiple (or at least one) reference translations for each segment. The reference translations are made by human translators. Multiple reference translations are used to account for the fact that there is no single correct translation.\n",
    "\n",
    "A machine translation system produces a hypothesis translation for each source segment. The hypothesis is then compared to all the references. BLEU looks for matching N-grams of different lengths N. The more matches, the better. Additionally, BLEU compares the length of the hypothesis with the lengths of the translations - brevity is penalized. These two components are combined into a score between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd14ed4a52b7d407d59494a4fc0d0002",
     "grade": false,
     "grade_id": "cell-5a5cd9018823435c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Match N-grams (3 Points) <a class=\"anchor\" id=\"subtask1_1\"></a>\n",
    "Specifically, BLEU uses a concept called modified precision. The hypothesis n-grams can find a match in any reference - but if the same n-gram occurs multiple times in the hypothesis, the number of matches is clipped to the maximum number of times the n-gram occurs in any single reference. The intuition is simple - the references are likely to have many words and phrases in common. Without the clipping, repeating the same likely words in the hypothesis would yield an inflated BLEU score. The hypothesis \"the the the the the the the\" would find many matches from almost any set of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9107096988cfded90303e65a8eecf6d4",
     "grade": false,
     "grade_id": "cell-7830e8ece39c4315",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('furiously', 'sleep', 'ideas'), ('sleep', 'ideas', 'green'), ('ideas', 'green', 'colorless')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You may use this ngram forming implementation in your solution.\n",
    "This returns a generator. If you absolutely need a list, you can\n",
    "simply call\n",
    ">>> list(form_ngrams(...))\n",
    "\"\"\"\n",
    "from collections import deque\n",
    "\n",
    "def form_ngrams(tokens, n):\n",
    "    \"\"\"Forms all ngrams from tokens\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : iterable\n",
    "        Tokens to form n-grams from\n",
    "    n : int\n",
    "        The length of ngrams to form\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    tuple\n",
    "        Yields each ngram as a tuple of tokens.   \n",
    "    \"\"\"\n",
    "    window = deque(maxlen=n)\n",
    "    for i, token in enumerate(tokens, start=1):\n",
    "        window.append(token)\n",
    "        if i >= n:\n",
    "            yield tuple(window)\n",
    "            \n",
    "print(list(form_ngrams([\"furiously\", \"sleep\", \"ideas\", \"green\", \"colorless\"], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c437283958ad8f06d1bee2e5674be686",
     "grade": false,
     "grade_id": "cell-c17a696f8678fb21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def modified_precision(hypothesis, references, n):\n",
    "    \"\"\"Matches hypothesis N-grams to references at given length N\n",
    "    \n",
    "    Hypothesis and references are given as plain token sequences. This function\n",
    "    forms N-grams of appropriate lengths.\n",
    "    \n",
    "    Count the number of times each N-gram occurs in the hypothesis.\n",
    "    Then take the union of N-gram counts in all the references:\n",
    "    for each N-gram, keep track of the maximum number of times it occurs in a single\n",
    "    reference. This forms the reference counts.\n",
    "    Finally, count the clipped hits - sum each N-gram count in the hypothesis,\n",
    "    but clip those hypothesis counts to the reference counts.\n",
    "    \n",
    "    This returns the hits and number of candidates separately, since those values\n",
    "    are aggregated over the full corpus in BLEU.\n",
    "    The modified precision for this single segment would be clipped_hits / num_candidates\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of candidate N-grams of length N in the hypothesis that found\n",
    "        a match.\n",
    "    int\n",
    "        The total number of candidate N-grams of length N in the hypothesis.\n",
    "    \"\"\"\n",
    "    hyp_counts = Counter(form_ngrams(hypothesis,n))\n",
    "    num_candidates = sum(hyp_counts.values())\n",
    "    if num_candidates == 0:\n",
    "        # If hypothesis is shorter than N, we get here.\n",
    "        # We return 0, 1 instead of 0, 0, so that we don't\n",
    "        # run into divide-by-zero errors.\n",
    "        return 0, 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # Initialize reference counts for maximum occurrences of each n-gram across all references\n",
    "    max_ref_ngram_counts = Counter()\n",
    "    \n",
    "    # Calculate maximum n-gram counts across all references\n",
    "    for reference in references:\n",
    "        ref_ngrams = Counter(form_ngrams(reference, n))\n",
    "        for ngram in ref_ngrams:\n",
    "            max_ref_ngram_counts[ngram] = max(max_ref_ngram_counts.get(ngram, 0), ref_ngrams[ngram])\n",
    "    \n",
    "    # Calculate clipped hits\n",
    "    clipped_hits = 0\n",
    "    for ngram, count in hyp_counts.items():\n",
    "        clipped_hits += min(count, max_ref_ngram_counts.get(ngram, 0))\n",
    "    \n",
    "    # Return the numerator and denominator separately, since\n",
    "    # they get aggregated over the whole corpus.\n",
    "    return clipped_hits, num_candidates\n",
    "\n",
    "def form_ngrams(tokens, n):\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80b30f9e5f609aba6edd595b534b098",
     "grade": true,
     "grade_id": "cell-abdd55f7362d275f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic sanity check:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 3), (2, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"d\"]], 4), (1, 1))\n",
    "\n",
    "# Small difference:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 1), (3, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 2), (2, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"]], 4), (0, 1))\n",
    "\n",
    "# More references:\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (4, 4))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (3, 3))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (1, 2))\n",
    "assert_equal(modified_precision([\"a\",\"b\",\"c\",\"d\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# Clipping:\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 1), (2, 4))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 2), (1, 3))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 3), (0, 2))\n",
    "assert_equal(modified_precision([\"c\",\"c\",\"c\",\"c\"], [[\"a\",\"b\",\"c\",\"e\"],[\"b\",\"c\",\"c\",\"d\"]], 4), (0, 1))\n",
    "\n",
    "# With real text:\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"], \n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "\n",
    "austen_hypo = [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "              \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "              \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]\n",
    "austen_ref = [[\"it\", \"is\", \"a\", \"truth\", \"universally\", \"acknowledged\",\n",
    "               \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\", \"be\", \"in\", \"want\", \"of\", \"a\", \"wife\"],\n",
    "              [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 3), (4,15))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 4), (2,14))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 5), (1,13))\n",
    "assert_equal(modified_precision(austen_hypo, austen_ref, 6), (0,12))\n",
    "\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                1), (2, 3))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                2), (0, 2))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                3), (0, 1))\n",
    "assert_equal(modified_precision([\"waters\", \"destroyed\", \"perfume\"],\n",
    "                                [[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "                                4), (0, 1))\n",
    "\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 1), (2, 7))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 2), (0, 6))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 3), (0, 5))\n",
    "assert_equal(modified_precision([\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "                                [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], \n",
    "                                 [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]], 4), (0, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9957de7709beea78c09779695d8c0000",
     "grade": false,
     "grade_id": "cell-68c5e02f9fade5ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Match lengths (1 Point) <a class=\"anchor\" id=\"subtask1_2\"></a>\n",
    "\n",
    "BLEU penalizes short hypotheses. This counter part is needed for precision, because precision by itself encourages adding only the most certain candidates. Typically recall is used as precision's pair, why not here? You can read the original paper to see. Instead of recall, BLEU uses a Brevity Penalty, which is computed over the whole test corpus. Here we just find the values which are aggregated - the hypothesis length and the closest reference length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78ffbf4094d18eee16f0c831294fa900",
     "grade": false,
     "grade_id": "cell-137382ee46d4f47f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def brevity_penalty_values(hypothesis, references):\n",
    "    \"\"\"Computes the length of the hypothesis and find the closest reference length.\n",
    "    \n",
    "    Pick the shortest reference length if there are many equally close\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    hypothesis : list of str\n",
    "        List of tokens - the hypothesis translation.\n",
    "        E.G. [\"alo\", \"mundo\"]\n",
    "    references : list of lists of str\n",
    "        List of references - and each reference is a list of tokens.\n",
    "        E.G. [[\"hola\", \"mundo\"], [\"alo\", \"tierra\"]]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The length of the hypothesis\n",
    "    int\n",
    "        The length of reference, which is closest to the hypothesis in length\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    This just considers length in tokens - this doesn't consider N-grams.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    hyplen = len(hypothesis)\n",
    "    closest_reflen = min(references, key=lambda ref: (abs(len(ref) - hyplen), len(ref)))\n",
    "    closest_reflen = len(closest_reflen)\n",
    "    \n",
    "    return hyplen, closest_reflen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d81ce1f3d284ab75009690e15caadf1",
     "grade": true,
     "grade_id": "cell-31315b4d20e99e6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic cases:\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\"],[\"a\",\"b\"]]), (3,3))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (3,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"],[\"a\",\"b\"]]), (4,2))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\"]]), (4,4))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\"], [[\"e\",\"f\",\"g\",\"h\"],[\"a\",\"b\",\"c\",\"d\",\"e\"]]), (2,4))\n",
    "assert_equal(brevity_penalty_values([], [[\"a\",\"b\",\"c\",\"d\",\"e\"],[]]), (0,0))\n",
    "assert_equal(brevity_penalty_values([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\"],\n",
    "                                    [[\"r\",\"s\",\"t\",\"u\",\"v\",\"x\",\"y\"],\n",
    "                                     [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\"]]), \n",
    "             (17, 22))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "488a7d02b8a0a849ffc466d4b603c59e",
     "grade": false,
     "grade_id": "cell-771b3a9511d24d16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Put it together <a class=\"anchor\" id=\"subtask1_3\"></a>\n",
    "\n",
    "The key in BLEU is to aggregate the statistics from above over a large test corpus. BLEU computed on an individual sentence does not correlate well with human scores. Even if this sentence-wise BLEU is computed for all individual sentences in a test corpus and then averaged, the resulting metric is much less useful than when a single BLEU score is computed over the full corpus.\n",
    "\n",
    "The modified precision aggregation happens at each N-gram length separately. Typically, BLEU-4 is computed, meaning maximum N-gram length 4. In that case, 8 precision values are collected: total number of hits and total number of candidates at each each N.\n",
    "\n",
    "The length values just consider length in tokens, so only two values are needed: the total length of the hypotheses and the total length of the best matching reference lengths. The brevity penalty (BP) is then computed as follows:\n",
    "$$\n",
    "BP = 1,~c>r\n",
    "$$\n",
    "$$\n",
    "BP = exp(1-\\dfrac{r}{c}), c\\leq r\n",
    "$$\n",
    "with aggregated hypothesis length $c$ and aggregated closest reference length $r$.\n",
    "\n",
    "The metric is finally computed by:\n",
    "$$\n",
    "BLEU = BP\\times exp\\left( \\sum_n^N \\left[\\dfrac{1}{N}log(p_n)\\right]\\right)\n",
    "$$\n",
    "\n",
    "Here aggregated modified precision at N-gram length $n$ is simply computed as: $p_n = \\dfrac{\\text{Clipped Hits}}{\\text{Total Candidates}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9440b0b38a3363e2c0d8863bed6475b3",
     "grade": false,
     "grade_id": "cell-699f52568aa9243a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use these exp and log operations:\n",
    "from math import exp, log\n",
    "EPSILON = 1e-10  # This is used for smoothing zero counts (already implemented)\n",
    "\n",
    "def BLEU(hypothesis_set, references_set, max_n=4):\n",
    "    \"\"\"Computes BLEU over a corpus\n",
    "    \n",
    "    Use your two earlier functions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect these statistics over the corpus ...\n",
    "    hits_totals = {N: 0 for N in range(1,max_n+1)} # Hits by N-gram N\n",
    "    num_candidates_totals = {N: 0 for N in range(1,max_n+1)} # Num candidates by N\n",
    "    hyplen_total = 0\n",
    "    closest_reflen_total = 0\n",
    "    # ... in the following loop:\n",
    "    for hypothesis, references in zip(hypothesis_set, references_set):\n",
    "        # YOUR CODE HERE \n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        hyplen, closest_reflen = brevity_penalty_values(hypothesis, references)\n",
    "        hyplen_total += hyplen\n",
    "        closest_reflen_total += closest_reflen\n",
    "\n",
    "        for N in range(1, max_n+1):\n",
    "            hits, num_candidates = modified_precision(hypothesis, references, N)\n",
    "            hits_totals[N] += hits\n",
    "            num_candidates_totals[N] += num_candidates\n",
    "            \n",
    "    # Additionally in BLEU, substituting a low value for zero counts is used\n",
    "    # to avoid log(0) problems.\n",
    "    # Smooth zero counts:\n",
    "    hits_totals = {N: hits if hits > 0 else EPSILON for N, hits in hits_totals.items()}\n",
    "    \n",
    "    # Now, compute brevity penalty based on \n",
    "    # hyplen_total and closest_reflen_totals\n",
    "    # Corner case: if hyplen_total == 0: brevity_penalty = 0.\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    if hyplen_total == 0:\n",
    "        brevity_penalty = 0\n",
    "    else:\n",
    "        brevity_penalty = exp(1 - closest_reflen_total / hyplen_total) if hyplen_total < closest_reflen_total else 1\n",
    "\n",
    "        \n",
    "    # Finally compute the precisions, and weight them uniformly by 1 / max_n\n",
    "    # Then multiply by brevity penalty and return the final score.\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    log_precision_sum = sum((log(hits_totals[N] / num_candidates_totals[N]) for N in range(1, max_n+1))) / max_n\n",
    "    BLEU_score = brevity_penalty * exp(log_precision_sum)\n",
    "    return BLEU_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1847bfb3a819737add2c376052f7f0c",
     "grade": true,
     "grade_id": "cell-3317cee95e9eda22",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# Basic corpus input:\n",
    "hyps = [[\"a\", \"b\", \"c\",\"d\"],\n",
    "        [\"colourless\", \"green\", \"ideas\"],\n",
    "       [\"a\",\"c\",\"d\"]]        \n",
    "references = [[[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]], \n",
    "              [[\"ideas\", \"green\", \"colourless\", \"sleep\"],[\"colourless\", \"green\"]],\n",
    "              [[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"c\",\"d\",\"e\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.5873949094699213, 2)\n",
    "# More corpus input:\n",
    "hyps = [[\"waters\", \"destroyed\", \"perfume\"],\n",
    "        [\"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\"],\n",
    "        [\"it\", \"is\", \"a\", \"widely\", \"accepted\", \"truth\", \n",
    "         \"that\", \"an\", \"unmarried\", \"wealthy\", \"man\", \n",
    "         \"necessarily\", \"needs\", \"a\", \"wife\", \"alongside\", \"him\"]]        \n",
    "references = [[[\"the\", \"city\", \"of\", \"cologne\", \"was\", \"destroyed\", \"by\", \"flood\", \"waters\"]], \n",
    "              [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]],\n",
    "              [[\"it\",\"is\",\"a\",\"truth\",\"universally\",\"acknowledged\",\n",
    "                \"that\", \"a\", \"single\", \"man\", \"in\", \"posession\", \"of\",\"a\"\"good\",\"fortune\",\n",
    "               \"must\",\"be\",\"in\",\"want\",\"of\",\"a\",\"wife\"],\n",
    "               [\"everybody\", \"knows\", \"the\",\"rich\",\"man\",\"necessarily\", \"needs\", \"a\", \"wife\"]]]\n",
    "assert_almost_equal(BLEU(hyps, references), 0.1502348037292212, 2)\n",
    "# Exactly the same:\n",
    "assert_equal(BLEU([[1,2,3,4]], [[[1,2,3,4]]]), 1.0)\n",
    "# Slightly different:\n",
    "assert_almost_equal(BLEU([[5,1,2,3,4]], [[[1,2,3,4,5]]]), 0.7071067811865475, 2)\n",
    "# No hypothesis:\n",
    "assert_equal(BLEU([[]], [[[\"Silence\"]]]), 0.)\n",
    "# No hits at N=4:\n",
    "assert_almost_equal(BLEU([[1,2,3,4]], [[[1,2,3,5]]]), 0.0022360679774997894, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0995f53aee1f243e10fc793c40a728f6",
     "grade": false,
     "grade_id": "cell-df2e90f797befcaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Reflect on BLEU <a class=\"anchor\" id=\"subtask1_4\"></a>\n",
    "Briefly answer the following questions:\n",
    "\n",
    "(Each answer should be **no longer than 5 sentences and must not exceed 150 words**. Be aware that answers surpassing these limits may result in a deduction of points.)\n",
    "\n",
    "1. Will providing more references increase the BLEU score? Why?\n",
    "\n",
    "2. Why is the brevity penalty is used instead of the more traditional recall?\n",
    "\n",
    "3. Will a human translator always get a BLEU score of 1? Why?\n",
    "\n",
    "4. Autograding these written questions is impossible. Or is it? How could we use BLEU to autograde these answers? The upside would be to save a lot of teaching assistant time, but what would be some downsides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "324538de940f16008e5f853338854de3",
     "grade": true,
     "grade_id": "cell-c038d749e95ea6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**1. Will providing more references increase the BLEU score? Why?**\n",
    "\n",
    "Yes, providing more references can increase the BLEU score. This is because BLEU measures accuracy by comparing the n-gram matches between the references and the translation (or hypothesis). Higher n-gram accuracy scores means it is likely that different formulations of the same concept found in the hypothesis would find a match in one of the references when there are more references. More references can also help us find a reference length closer to the hypothesis length and thus lowers the brevity penalty. However, after a certain number of references is added , the BLUE score improvement may reach a plateau and it no longer increases.\n",
    "\n",
    "**2. Why is the brevity penalty is used instead of the more traditional recall?**\n",
    "\n",
    "The brevity penalty is used in BLEU instead of recall to deal with the issue of translation extreme brevity. Recall measures how many of the correct n-grams are in the hypothesis from all possible correct n-grams from the references, which could favor longer translations that cover more reference n-grams, regardless of their relevance or correctness. However, BLEU tries to evaluate the quality of machine translations by penalizing very short translations. The brevity penalty ensures that translations are not only precise but also closer in length to the reference translations. \n",
    "\n",
    "=> Brevity penalty is used instead of recall to ensure machine translations are not too short, nor too long due to the recall metrics that prefers longer translations.  \n",
    "\n",
    "**3. Will a human translator always get a BLEU score of 1? Why?**\n",
    "\n",
    "No, a human translator will not always get a BLEU score of 1. The BLEU score measures the similarity between a machine-generated translation and a referenced translation by humans based on the n-grams count. However, even high-quality human translations can choose different wording or expressions that do not match those in the reference translations, leading to less than perfect n-gram overlapping. Additionally, the BLEU score has a brevity penalty that penalizes translations for being too short no matter their quality. Since there is often more than one valid way to translate a text, another human translated version might differ from the reference version in length and word choices, thus not achieving a BLEU score of 1.\n",
    "\n",
    "**4. Autograding these written questions is impossible. Or is it? How could we use BLEU to autograde these answers? The upside would be to save a lot of teaching assistant time, but what would be some downsides?**\n",
    "\n",
    "- Autograding written questions with BLEU is possible but there are some significant downsides when we use it. \n",
    "\n",
    "- For BLEU score to be applied, it should compare student answers to high-quality reference answers produced by the course staff, which will reduce grading time by the TAs. \n",
    "\n",
    "- The key downside is BLEU's focus on exact n-gram matches, which may not accurately contain the original expression in student responses. This method will unfortunately penalize varied but still otherwise valid answers that do not match the reference wording closely.\n",
    "\n",
    "- Another downside is that BLEU cannot assess the correctness of content aside from lexical similarity, meaning it might miss a serious misconception in student answers that are somehow phrased similarly to the reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
