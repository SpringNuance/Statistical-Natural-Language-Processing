{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cc975cc1ab967bb9819e0a807ff490b",
     "grade": false,
     "grade_id": "cell-3f2c101d987217b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 1: Text Preprocessing\n",
    "\n",
    "## Released: 16.01.2024\n",
    "## Deadline: 26.01.2024 at 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "457695a4f4ceed6349c5c2c60a978c9c",
     "grade": false,
     "grade_id": "cell-a1a6fd2cf64b99ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "Consider this assignment an introduction to statistics of different language units (letters, pairs of letters, words). We will explore the frequency distribution of these different language units and then discuss what this knowledge might give. Moreover, we'll talk about how to handle raw text, how to separate it into different units and how those units and operations are called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdecf61523ef597245be1f71270f06e4",
     "grade": false,
     "grade_id": "cell-60bb5ff6140a29ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: Letter and Letter pair Frequency analysis](#task_1)\n",
    "    * [Step 1.1: Prepare the text (1 point)](#subtask_1_1)\n",
    "    * [Step 1.2: Get letter frequencies (3 points)](#subtask_1_2)\n",
    "    * [Step 1.3: Letter frequency analysis (1 point)](#subtask_1_3)\n",
    "    * [Step 1.4: Count all possible two-letter strings (1 point)](#subtask_1_4)\n",
    "    * [Step 1.5: Get letter pair counts (3 points)](#subtask_1_5)\n",
    "    * [Step 1.6: Letter pair frequency analysis (3 points)](#subtask_1_6)\n",
    "* [Task 2: Word Tokenization](#task_2)\n",
    "    * [Step 2.1: Tokenize by whitespaces (1 point)](#subtask_2_1)\n",
    "    * [Step 2.2: Tokenize with regular expressions (5 points)](#subtask_2_2)\n",
    "    * [Step 2.3: Use Treebank tokenizer (1 point)](#subtask_2_3)\n",
    "* [Task 3: ](#task_3)\n",
    "    * [Step 3.1: Analyse word frequencies (3 points)](#subtask_3_1)\n",
    "    * [Step 3.2: Remove stop words (1 point)](#subtask_3_2)\n",
    "* [Checklist before submission](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d40f6bf6aa3a340761e99c9669da953",
     "grade": false,
     "grade_id": "cell-750a6466a51a9aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## Letter and Letter pair Frequency analysis\n",
    "\n",
    "The data used in this assignment is \"The Gold-Bug\" by Edgar Allan Poe. It is actually a story about the importance of letter frequencies. The narrator in the story was able to decipher a message leading to a hidden treasure by applying frequency analysis. The cipher used in the story is a substitute cipher where each letter is replaced by a different letter or number.\n",
    "\n",
    "Knowing the frequency of letters in a language is important not only for solving ciphers, but it also has practical applications like data compression. For example, Morse code uses the shortest symbols for the most frequent letters. \n",
    "\n",
    "For the purposes of this assignment, \"The Gold-Bug\" text serves as a representation of the English language. While the text is short, the statistics you will compute from it in this assignment reflect those of the English language in general. You will not need any external data to answer the questions in this assigment.\n",
    "\n",
    "In the first task you'll need to discover for yourself the frequency distribution of single letters and of letter pairs in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee6bd53bcbfab30f09c2f92a4328806c",
     "grade": false,
     "grade_id": "cell-d5d46f8ec022767a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.1  <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### Prepare the text (1 point)\n",
    "\n",
    "First of all, we need to load the text into the Jupyter Notebook and prepare it for further analysis. Create a function that reads the data located in `/coursedata/text-processing/the_gold-bug.txt`, lowercases it, and returns the text as a list of separate letters, ignoring all non-alphabetic characters.\n",
    "\n",
    "HINT1: [string methods might come in handy](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "HINT2: you can employ Python's open() and read() functions\n",
    "\n",
    "HINT3: Alphabetic characters are characters defined as “Letter” in the Unicode character database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8cf3bc57d21b0f61a80c51a84776b43",
     "grade": false,
     "grade_id": "cell-6027b855ab1ece6c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', 'g', 'o', 'l', 'd', 'b', 'u', 'g', 'w', 'h', 'a', 't', 'h', 'o', 'w', 'h', 'a', 't', 'h', 'o', 't', 'h', 'i', 's', 'f', 'e', 'l', 'l']\n"
     ]
    }
   ],
   "source": [
    "def read(file_name):\n",
    "    \"\"\"This function creates a list of lowercase alphabetic characters from a .txt file \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to the text file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lowercased_letters : list of strings\n",
    "        text as a list of lowercase letters\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    with open(file_name, encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        lowercased_letters = [char.lower() for char in text if char.isalpha()]    \n",
    "\n",
    "    return lowercased_letters\n",
    "\n",
    "bug_file_path = '../coursedata/text-processing/the_gold-bug.txt'\n",
    "bug_letters = read(bug_file_path)\n",
    "print(bug_letters[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce651a7389744217b7c201f3e6c7c26a",
     "grade": true,
     "grade_id": "cell-a6ece3ca5d559fd1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nose in /opt/software/lib/python3.10/site-packages (1.3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install nose\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# checks if your function returns a list\n",
    "assert_equal(type(read(bug_file_path)), list)\n",
    "\n",
    "# checks if your function returns a list of strings\n",
    "assert_equal(type(read(bug_file_path)[0]), str)\n",
    "\n",
    "# checks if your list is of the right length\n",
    "assert_equal(len(read(bug_file_path)), 58269)\n",
    "\n",
    "# checks if the strings in your list are alpabetic characters\n",
    "assert_equal(all([letter.isalpha() for letter in read(bug_file_path)]), True)\n",
    "\n",
    "# checks if the strings in your list are lowercased\n",
    "assert_equal(all([letter.islower() for letter in read(bug_file_path)]), True)\n",
    "\n",
    "# checks if your list has the first 10 members right \n",
    "assert_equal(read(bug_file_path)[:10],\n",
    "             ['t', 'h', 'e', 'g', 'o', 'l', 'd', 'b', 'u', 'g'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0049753342aad8fa5ac1ee09b9a4a5b6",
     "grade": false,
     "grade_id": "cell-0dc5934ca1a0e8dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### Get letter frequencies (3 points)\n",
    "\n",
    "Now we can count how many times each letter occurred in the story, and then turn these counts into Maximum Likelihood probability estimates of seeing each letter. \n",
    "\n",
    "To do so, write a function that takes in a list of letters and returns a dictionary with their frequencies relative to the size of the whole text: $ freq_x = \\frac{n_x}{N} $, where $n_x$ is the number of times a letter $x$ was seen, and $N$ is the total number of letters in the text. These relative frequencies are probability estimates for the letters in our text.\n",
    "\n",
    "HINT: you might find **nltk.FreqDist** or **collections.Counter** useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91edaad811e9fce2ae86d68ab9068832",
     "grade": false,
     "grade_id": "cell-a3e7f79d12bc50df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 0.09413238600284886, 'h': 0.05786953611697472, 'e': 0.13125332509567694, 'g': 0.01961591927096741, 'o': 0.07221678765724485, 'l': 0.03988398633921982, 'd': 0.04347079922428736, 'b': 0.01769379944739055, 'u': 0.03248725737527673, 'w': 0.022361804733220067, 'a': 0.07722802862585594, 'i': 0.07178774305376787, 's': 0.06034083303300211, 'f': 0.023889203521598106, 'n': 0.06713689955207744, 'c': 0.026137397243817466, 'm': 0.025725514424479567, 'y': 0.019667404623384645, 'r': 0.056256328407901283, 'v': 0.009009936673016528, 'q': 0.0010297070483447459, 'p': 0.020062125658583466, 'k': 0.006023786232816764, 'x': 0.0020594140966894918, 'z': 0.0007551185021194804, 'j': 0.0019049580394377799}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def get_freq(letters):\n",
    "    \"\"\"This function computes MLE probabilities of letters\n",
    "    \n",
    "    Given a list of letters, this function should return a probability dictionary,\n",
    "    where letters are keys and their MLE probabilities are values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    letters : list of strings\n",
    "        text as a list of only lowercase letters \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    letter_freq_dict : dictionary-like object\n",
    "        a frequency dictionary of letters where values are relative frequencies (MLE probabilities)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    freq_dist = FreqDist(letters)\n",
    "    total_letters = len(letters)    \n",
    "    # Convert the counts to probabilities\n",
    "    letter_freq_dict = {letter: freq / total_letters for letter, freq in freq_dist.items()}\n",
    "    return letter_freq_dict\n",
    "\n",
    "bug_probability_dict = get_freq(bug_letters)\n",
    "print(bug_probability_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2cf7330c7193edea1d2d06e4b9d7816",
     "grade": true,
     "grade_id": "cell-a53de57ea983102e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "# checks if the alphabet length is 26\n",
    "assert_equal(len(get_freq(bug_letters)), 26)\n",
    "\n",
    "# checks if probability of all the letters equals one\n",
    "assert_almost_equal(sum(get_freq(bug_letters).values()), 1., 3)\n",
    "\n",
    "# checks if the algorithm is doing what it is supposed to be doing on a dummy example\n",
    "assert_equal(get_freq(['b','b','b','a','a','c']), {\"b\":3/6,\"a\":2/6,\"c\":1/6})\n",
    "\n",
    "# checks if the probability of 'e' is correct\n",
    "assert_almost_equal(get_freq(bug_letters)['e'], 0.13125332509567694, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09aaef2c564e649ffc73dc284b6d4029",
     "grade": false,
     "grade_id": "cell-247e03c8c52f0e3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "### Letter frequency analysis (1 point)\n",
    "The counts of letters in a text differ quite much. That means the probabilities of seeing each letter are also different.\n",
    "\n",
    "Using your frequency dictionary, answer the following questions:\n",
    "\n",
    "1. What is the probability of the most frequent letter? (0.3 points)\n",
    "2. What is the least probable letter? (0.3 points)\n",
    "3. What is the order of English letters according to their probability? Answer with a sorted string of letters starting with the most frequent one. (0.4 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed. If the answer is a float, then always insert the full float into the variable or the code that produces the correct float. Floats are checked up to 3 decimal points so there is some room for floating point errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95dba934b155fac867d51c27fc212a5a",
     "grade": false,
     "grade_id": "cell-7f9e8133be37542f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent letter probability:\n",
      " 0.13125332509567694\n",
      "The least frequent letter probability:\n",
      " z\n",
      "sorted letters (most to least frequent):\n",
      " etaoinshrdlucmfwpygbvkxjqz\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "sorted_bug_probability_dict = dict(sorted(bug_probability_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"The most frequent letter probability:\\n\", list(sorted_bug_probability_dict.values())[0])\n",
    "print(\"The least frequent letter probability:\\n\", list(sorted_bug_probability_dict.keys())[-1])\n",
    "print(\"sorted letters (most to least frequent):\\n\", ''.join(list(sorted_bug_probability_dict.keys())))\n",
    "\n",
    "# put your answer to question 1 as a float number into the variable below\n",
    "# For example:\n",
    "# most_frequent_letter_prob = 0.12345\n",
    "most_frequent_letter_prob = None ## FILL IN THE ANSWER\n",
    "most_frequent_letter_prob = 0.13125332509567694 ## FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 2 as a string into the variable below\n",
    "# For example:\n",
    "# least_probable_letter = 'a'\n",
    "least_probable_letter = 'z' ## FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 3 as a string into the variable below\n",
    "# For example:\n",
    "# sorted_letters = 'abcdefghigklmnopqrstuvwxyz'\n",
    "sorted_letters = 'etaoinshrdlucmfwpygbvkxjqz' ## FILL IN THE ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2fa8dd98e9de235bfa26acadc3fbe2a",
     "grade": true,
     "grade_id": "cell-57d24f0b13fea3d7",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(most_frequent_letter_prob), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0c01b73ca9119ace9b57266958a2d3d",
     "grade": true,
     "grade_id": "cell-8082c3b77391c5f9",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a string\n",
    "assert_equal(type(least_probable_letter), str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f4b4e98270e90ecbdc5e766c9f8891a",
     "grade": true,
     "grade_id": "cell-de8d57ffd847e5d6",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if you typed in a string of 26 letters\n",
    "assert_equal(type(sorted_letters), str)\n",
    "assert_equal(len(sorted_letters), 26)\n",
    "# checks if the 3rd most frequent letter is 'a'\n",
    "assert_equal(sorted_letters[2], 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef44dfbf97d819329a74fd37e249d542",
     "grade": false,
     "grade_id": "cell-ff1e21f485bb65ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.4 <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "### Count all possible two-letter strings (1 point)\n",
    "Similar to single letters, some combinations of language units are more likely than others. You'll see it in a minute.\n",
    "\n",
    "There are 26 letters in the English alphabet. How many possible two-letter strings are there according to combinatorics ('permutations' combinatorically speaking)? For example, if we have an alphabet of 3 letters **a**, **b** and **c**, we can have 9 two-letter strings: **aa**, **bb**, **cc**, **ab**, **ac**, **ba**, **bc**, **ca**, **cb**.\n",
    "\n",
    "Complete the function below, so we can count all n-length strings for an alphabet of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2057fd6c9a5af7b45e0facc95f5737a",
     "grade": false,
     "grade_id": "cell-e8239fe099ac2509",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def number_of_permutations(number_of_letters, sequence_len):\n",
    "    \"\"\"Counts the number of possible letter permutations in a string.\n",
    "    \n",
    "    This function takes the number of letters in an alphabet and the desired length of a string,\n",
    "    and outputs the number of all possible letter permutations in a string of this length.\n",
    "    A string can contain the same letter \"sequence_len\" times.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    number_of_letters : int \n",
    "        a number of letters in an alphabet\n",
    "    sequence_len : int\n",
    "        the length of a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    num_of_permutations : int\n",
    "        the number of all possible strings of a given length with a given alphabet\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    num_of_permutations = number_of_letters ** sequence_len\n",
    "    return num_of_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7976d78b2818a5ee57c0f241a911775",
     "grade": true,
     "grade_id": "cell-305f8cc935f8beab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_equal(number_of_permutations(2,2), 4)\n",
    "assert_equal(number_of_permutations(2,1), 2)\n",
    "assert_equal(number_of_permutations(3,2), 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a563ab41061cd5f009b18e96cb65921a",
     "grade": false,
     "grade_id": "cell-b8c1ea1279ede03e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.5 <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "### Get letter pair counts (3 points)\n",
    "\n",
    "In previous task, you computed the total number of possible permutations, where two-lettered strings were given as an example. However, not all strings of two letters actually appear in English. A fact like this can be used in such applications as predictive text: your phone suggests what might be the next word you need based on previous words you typed (not all word sequences are possible or equally probable).\n",
    "\n",
    "In the following task, you'll need to count all two-letter strings that appear in the given Gold-Bug text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace0a1f2836699264cb71470c5fd13c1",
     "grade": false,
     "grade_id": "cell-faf866b8fc92dc87",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('th', 1800), ('he', 1432), ('in', 1099), ('re', 935), ('er', 924), ('an', 858), ('ed', 790), ('en', 726), ('es', 692), ('nd', 676), ('ea', 666), ('nt', 662), ('at', 646), ('it', 642), ('ha', 628), ('on', 614), ('ou', 603), ('te', 582), ('ti', 567), ('st', 567)]\n"
     ]
    }
   ],
   "source": [
    "def count_and_sort_pairs(letters):\n",
    "    \"\"\"This function counts letter pairs and sorts them according to their frequency.\n",
    "    \n",
    "    This function takes a text represented as a list of lowercase letters\n",
    "    and converts it into a sorted list of tuples, where the first element\n",
    "    is a two-letter string, and the second element is the count of this letter pair in the text.\n",
    "    The first element of a list should be a tuple for the most frequent pair.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    letters : list of strings\n",
    "        text as a list of lowercase letters \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pairs_sorted : list of (str, int) tuples\n",
    "        a list of tuples (letter_pair, count) sorted by the count element \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    letters_string = ''.join(letters)\n",
    "    # Count all possible two-letter pairs using a sliding window approach\n",
    "    letter_pairs = [letters_string[i:i+2] for i in range(len(letters_string)-1)]\n",
    "    letter_pairs_freq = Counter(letter_pairs)\n",
    "    pairs_sorted = sorted(letter_pairs_freq.items(), key=lambda pair: pair[1], reverse=True)\n",
    "    return pairs_sorted\n",
    "\n",
    "bug_pairs_sorted = count_and_sort_pairs(bug_letters)\n",
    "\n",
    "print(bug_pairs_sorted[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4f34badcf4661bb1047fefac0458270",
     "grade": true,
     "grade_id": "cell-a02209796380d0cc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if the function returns a list\n",
    "assert_equal(type(count_and_sort_pairs(bug_letters)), list)\n",
    "\n",
    "# checks if the function returns a list of tuples\n",
    "assert_equal(type(count_and_sort_pairs(bug_letters)[0]), tuple)\n",
    "\n",
    "# checks if the function returns a list of tuples (str, int)\n",
    "assert_equal((type(count_and_sort_pairs(bug_letters)[0][0]),type(count_and_sort_pairs(bug_letters)[0][1])), (str,int))\n",
    "\n",
    "# checks that the most frequent pair was seen 1800 times\n",
    "assert_equal(count_and_sort_pairs(bug_letters)[0][1], 1800)\n",
    "\n",
    "# checks if a functions works right for the dummy example\n",
    "assert_equal(count_and_sort_pairs(\"aaaabbabc\")[:2], [(\"aa\",3),(\"ab\",2)])\n",
    "assert(count_and_sort_pairs(\"aaaabbabc\")[2] in [(\"ba\",1), (\"bb\", 1), (\"bc\", 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "454edcafd5ef9cd41972e54f2e6c2f44",
     "grade": false,
     "grade_id": "cell-dcb81e3883d219e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.6 <a class=\"anchor\" id=\"subtask_1_6\"></a>\n",
    "### Letter pair frequency analysis (3 points)\n",
    "Using the sorted list you've created (`bug_pairs_sorted`), answer the following questions in the cell below:\n",
    "\n",
    "1. How many different two-letter combinations have you actually encountered in the data? (0.6 points)\n",
    "2. What fraction of all theoretically possible two-letter strings (i.e. permutations) is it? (0.6 points)\n",
    "3. What is the most frequent two-letter string in English (make a mental note if it is surprising or not)? The Gold-Bug text serves here as a proxy for the English language. (0.6 points)\n",
    "4. What is the probability of seeing a pair where both letters are the same? (0.6 points)\n",
    "5. What is the probability of a pair starting with 'm'? (0.6 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffdcc61151d2fc77e84a403e722a5918",
     "grade": false,
     "grade_id": "cell-f59c76bc63d556f5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different two-letter combinations: 519\n",
      "Fraction of all theoretically possible two-letter strings: 0.7677514792899408\n",
      "Most frequent two-letter string: th\n",
      "Probability of seeing a pair where both letters are the same: 0.03459875060067275\n",
      "Probability of a pair starting with 'm': 0.025725955927781975\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "n_pairs = len(bug_pairs_sorted)\n",
    "\n",
    "frac_of_pairs = n_pairs / (26 * 26)\n",
    "\n",
    "most_frequent_pair = bug_pairs_sorted[0][0]\n",
    "\n",
    "total_counts = sum(count for pair, count in bug_pairs_sorted)\n",
    "p_same_letters = sum(count for pair, count in bug_pairs_sorted if pair[0] == pair[1]) / total_counts\n",
    "\n",
    "p_starts_with_m = sum(count for pair, count in bug_pairs_sorted if pair[0] == 'm') / total_counts\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of different two-letter combinations: {n_pairs}\")\n",
    "print(f\"Fraction of all theoretically possible two-letter strings: {frac_of_pairs}\")\n",
    "print(f\"Most frequent two-letter string: {most_frequent_pair}\")\n",
    "print(f\"Probability of seeing a pair where both letters are the same: {p_same_letters}\")\n",
    "print(f\"Probability of a pair starting with 'm': {p_starts_with_m}\")\n",
    "\n",
    "\n",
    "# put your answer to question 1 as an int into the variable below\n",
    "# For example:\n",
    "# n_pairs = 1000\n",
    "n_pairs = 519 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 2 as a float into the variable below\n",
    "# For example:\n",
    "# frac_of_pairs = 0.12345\n",
    "frac_of_pairs = 0.7677514792899408 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 3 as a string into the variable below\n",
    "# For example:\n",
    "# most_frequent_pair = 'ab'\n",
    "most_frequent_pair = 'th' ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 4 as a float into the variable below\n",
    "# For example:\n",
    "# p_same_letters = 0.12345\n",
    "p_same_letters = 0.03459875060067275 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 5 as a float into the variable below\n",
    "# For example:\n",
    "# p_starts_with_m = 0.12345\n",
    "p_starts_with_m = 0.025725955927781975 ##FILL IN THE ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9bfbbd4c0e936eef40ce255f395405e",
     "grade": true,
     "grade_id": "cell-30230a0028dd5352",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_pairs) , int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69170691169c268d24aa4714f35a0dff",
     "grade": true,
     "grade_id": "cell-43fa0a46b8247515",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_of_pairs), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9874666bfb7aabc1a637c01188b64675",
     "grade": true,
     "grade_id": "cell-e8ed30f2e25d81d3",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a string\n",
    "assert_equal(type(most_frequent_pair),str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19a35d9b8a2cc60cfd247ef0d3ff1634",
     "grade": true,
     "grade_id": "cell-100f117c7f64f588",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(p_same_letters),float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e64dd0e26aa6835a8c7224137090d4f7",
     "grade": true,
     "grade_id": "cell-ee860b00f1a8983a",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(p_starts_with_m),float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e8dd238dd15aad32fbdce7969646775",
     "grade": false,
     "grade_id": "cell-3bfa916e5c65d448",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## Word Tokenization\n",
    "\n",
    "In this task, you will create a function that splits the text into more elaborate units than just letters: words.\n",
    "\n",
    "Text data is a part of virtually any NLP application. Sometimes you're lucky, and instead of plain raw text you get nice and clean text, but this is not always the case. Before getting your hands dirty with your actual application, you would most probably need to perform some manipulation of the text. For instance, separate it into words and sentences, and remove unwanted symbols. Different tasks require different preprocessing techniques. In this task we'll use some simple ones.\n",
    "\n",
    "It's not trivial to separate words from a string of text. The first thing that needs to be decided is what to count as a word. Should punctuation and numbers be considered words? Should *frogs* and *frog* be considered the same word? What about *Frog*, *frog* and *FROG*? Before answering those questions, let's make sure we are on the same page and discuss some terminology.\n",
    "\n",
    "When talking about words, we can mean several different things: lemmas, word types and word tokens.\n",
    "\n",
    "* **Lemma** - an identifier of a set of lexical forms sharing the same stem (*run* is the lemma for *runs* and *running*), a dictionary form of a word.\n",
    "* **Word type** - a distinct word in a text (all the instances of *runs* are counted as one word type).\n",
    "* **Word token** - every instance of word occurrence (every instance of *runs* counted as a separate word token).\n",
    "\n",
    "Thus:\n",
    "* **Tokenization** - a process of separating out word tokens from text\n",
    "* **Lemmatization** - a process of assigning a group of word forms their lemma, and further separating out these lemmas from text\n",
    "\n",
    "It is common to also widen the definition of 'word' to include punctuation.\n",
    "\n",
    "Generally, English doesn't require lemmatization since it has quite a limited number of word forms. For this reason, we'll leave this task out, for now, and focus on tokenization instead.\n",
    "\n",
    "Let's create a tokenizer that considers numbers and punctuation as tokens and doesn't separate hyphenated words like *dum-dum*. For that you'll need:\n",
    "- regular expressions\n",
    "- string operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a089fd4e5c20509826a1775bbf06883",
     "grade": false,
     "grade_id": "cell-91014079a96c570a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Tokenize by whitespaces (1 point)\n",
    "Let's start off by separating words just by whitespaces and see what happens to our dummy sentence example: \n",
    "*It's a dum-dum example, we'll place it here to prove a point. Also look at this number: 300.99.*\n",
    "\n",
    "HINT: One string method is particularly useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63df4a2b5642fe762a17b3157d650605",
     "grade": false,
     "grade_id": "cell-323fdf945301cde7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'dum-dum', 'example,', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point.', 'Also,', 'look', 'at', 'this', 'number:', '300.99.']\n"
     ]
    }
   ],
   "source": [
    "dummy_example = \"It's a dum-dum example, we'll place it here to prove a point. Also, look at this number: 300.99.\"\n",
    "\n",
    "def whitespace_tokenize(raw_string):    \n",
    "    \"\"\"This function tokenizes strings by whitespaces.\n",
    "    \n",
    "    Any whitespace separator should work. \n",
    "    For example, this function should be able to tokenize by '\\n',\n",
    "    and two consecutive whitespaces should be regarded as a single separator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_string : str\n",
    "        some text to tokenize\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    whitespace_tokenized : list of strings\n",
    "        list of tokens \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    whitespace_tokenized = raw_string.split()\n",
    "    return whitespace_tokenized\n",
    "\n",
    "dum_dum_example = whitespace_tokenize(dummy_example)\n",
    "# see what you've got\n",
    "print(dum_dum_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f1d24b7aeca9862d0cec387a5a0ae7a",
     "grade": true,
     "grade_id": "cell-f63d5a4956e3323c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if the first token is correct\n",
    "assert_equal(dum_dum_example[0], \"It's\")\n",
    "\n",
    "# checks if number of tokens is correct\n",
    "assert_equal(len(dum_dum_example), 18)\n",
    "\n",
    "# checks if all whitespaces were removed\n",
    "assert_equal(any([' ' in t for t in dum_dum_example]), False)\n",
    "\n",
    "# checks if double whitespaces are removed\n",
    "assert_equal(whitespace_tokenize('  a  a  '), ['a','a'])\n",
    "\n",
    "# checks if tab ia removed too\n",
    "assert_equal(whitespace_tokenize('  a \\t a  '), ['a','a'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65ad19ea848820b8d8e7c093ae0857c0",
     "grade": false,
     "grade_id": "cell-ad4e2d19588ca13a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.2 <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "### Tokenize with regular expressions (5 points)\n",
    "\n",
    "As can be seen from the dummy example, it's not enough to just separate words by the whitespaces. This way we get tokens like *'example,'*, *'point.'* and *'number:'*. It's not ideal because we would actually like to have punctuation marks as separate tokens but keep them inside the items like prices and numbers (4.99). Thus, we need something more complex: a regular expression.\n",
    "\n",
    "\n",
    "If you've never used regular expressions before or you've never used them in Python, you can read about how they work with the re module [here](https://docs.python.org/3/howto/regex.html). \n",
    "\n",
    "But to give you a simple example, you can think of a regular expression as a shoe that only fits some of the strings. For example, regular expression 'a' (`re.compile(\"a\")`) only fits a string 'a', but regular expression with a special character '\\d' (`re.compile(\"\\d\")`) fits any digit '0','1','2',...'9'. Regular expression 'a|\\d' (`re.compile(\"a|\\d\")`) fits to 'a' string OR to any string with a digit ('|' plays a role of disjunction). You can ask to find all the substrings of a string that match some regular expression pattern with the following piece of code: `re.findall(regular_expression, string_to_look_for_matches)`. More examples are in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb1151e43d761da56b0009042763b10",
     "grade": false,
     "grade_id": "cell-5cd677d2ecfbef29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '7']\n",
      "['a', 'a']\n",
      "['a1']\n",
      "['a', '1', '2', '3', '7', 'a']\n",
      "['123', '7']\n"
     ]
    }
   ],
   "source": [
    "# example of regular expression usage\n",
    "import re\n",
    "\n",
    "regex_digit = re.compile(\"\\d\") # any digit\n",
    "regex_a = re.compile(\"a\") # only letter 'a'\n",
    "regex_a_digit = re.compile(\"a\\d\") # letter 'a' followed by any digit \n",
    "regex_a_or_digit = re.compile(\"a|\\d\") # letter 'a' OR any digit \n",
    "regex_digit_once_or_more = re.compile(\"\\d+\") # any digit one or more times\n",
    "\n",
    "print(re.findall(regex_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_a_or_digit ,'a123hnd7hjaf'))\n",
    "print(re.findall(regex_digit_once_or_more ,'a123hnd7hjaf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "711f6ebbef8f999de2ee654086c747f2",
     "grade": false,
     "grade_id": "cell-2950c20ef363541c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can use regular expressions to describe what type of substring we want to get out of a string. In addition to separation by whitespaces, we want to keep words or numbers with a hyphen, an apostrophe or a point inside, and we want to split punctuation marks from the end of words. For these means, you'll need to write a regular expression that matches:\n",
    "\n",
    "- all alphanumeric strings with hyphen, apostrophe or point inside (i.e. should be able to find \"44.44\",\"a-ha\",\"it's\")\n",
    "\n",
    "**OR**\n",
    "- any non-whitespace character followed between zero and unlimited times by any alphanumeric character (i.e. \"hello!.?\" should result in \"hello\", \"!\", \".\" and \"?\".)\n",
    "\n",
    "HINT1: \"\\S\" - non-whitespace character \n",
    "\n",
    "HINT2: \"\\w\" - alphanumeric character\n",
    "\n",
    "HINT3: \"[123]\" - one of the characters in the brackets\n",
    "\n",
    "HINT4: \"*\" - zero or more times\n",
    "\n",
    "HINT5: more hints in this [cheat sheet](https://www.rexegg.com/regex-quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4234aa761445149765b405106603611c",
     "grade": false,
     "grade_id": "cell-3c1fce0956ef8f25",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'dum-dum', 'example', ',', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point', '.', 'Also', ',', 'look', 'at', 'this', 'number', ':', '300.99', '.']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "# put a compiled regular expression tokenizer into the variable below\n",
    "# For example:\n",
    "# regex_tokenizer = re.compile('\\w+')\n",
    "regex_tokenizer = re.compile('\\w+[-.\\']*\\w+|\\S') ## FILL IN THE ANSWER\n",
    "\n",
    "# look if our dummy example is now tokenized properly\n",
    "print(re.findall(regex_tokenizer, dummy_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdf73face8cfe363b03adf8888b31d9e",
     "grade": true,
     "grade_id": "cell-7936f42e9fa94a1e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if punctuation marks are separated from the end of a word\n",
    "assert_equal(re.findall(regex_tokenizer, \"hello!.?\"),['hello', '!', '.', '?'])\n",
    "\n",
    "# checks if punctuation marks are separated from the end of a word and words are whitespace separated\n",
    "assert_equal(re.findall(regex_tokenizer, \"well,  well, well\"),['well', ',', 'well', ',', 'well'])\n",
    "\n",
    "# checks if words and numbers with -'. inside are kept intact\n",
    "assert_equal(re.findall(regex_tokenizer, \"bye-bye, we'll call you at 3.15\"),\n",
    "             ['bye-bye', ',', \"we'll\", 'call', 'you', 'at', '3.15'])\n",
    "\n",
    "# checks if the last token in the dummy sentence is correct\n",
    "assert_equal(re.findall(regex_tokenizer, dummy_example)[-1], '.')\n",
    "\n",
    "# checks if number of tokens in the dummy sentence is correct\n",
    "assert_equal(len(re.findall(regex_tokenizer, dummy_example)), 23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abdb1684e147e8aba44b083f0b35e706",
     "grade": false,
     "grade_id": "cell-0868a2120ad8852f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.3 <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Use Treebank tokenizer (1 points)\n",
    "\n",
    "As you've already noticed, the process of creating a tokenizer is pretty complicated. There are many more things to consider, and a tokenizer should be chosen in accordance with a task. For example, we might also want to capture abbreviations (U.S.A.), percentages (82%) or URLs.\n",
    "\n",
    "Luckily, there are already several good tokenizers implemented for us. For instance, the NLTK package has several (https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize). \n",
    "\n",
    "Let's tokenize our text using the Treebank tokenizer. It uses regular expressions to tokenize text so that tokens match those used in a popular [Penn Treebank](https://web.archive.org/web/19970614160127/http://www.cis.upenn.edu/~treebank/) dataset. This tokenizer also assumes that the text has already been segmented into sentences. Perform sentence segmentation using NLTK's `sent_tokenize()`. Don't forget to lowercase the tokens after tokenizing. If you lowercase before tokenization, it may have an effect on the segmentation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfe9bcdad99414f58d89fb2237ba45c8",
     "grade": false,
     "grade_id": "cell-b1ec48f823b7c5c4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'a', 'dum-dum', 'example', ',', 'we', \"'ll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point.', 'Also', ',', 'look', 'at', 'this', 'number', ':', '300.99', '.']\n"
     ]
    }
   ],
   "source": [
    "# There were some issues related to SSL certificates when downloading\n",
    "# nltk library (punkt).\n",
    "# This is a workaround that disables SSL check.\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "\n",
    "# here is how the Treebank tokenizer handles the dummy sentence\n",
    "print(TreebankWordTokenizer().tokenize(dummy_example))\n",
    "\n",
    "def tokenize_and_lowercase(file_name):\n",
    "    \"\"\"This function tokenizes text files into lowercased tokens with TreebankWordTokenizer\n",
    "    \n",
    "    Read a text file into a string,\n",
    "    tokenize this string into sentences using sent_tokenize(),\n",
    "    tokenize each sentence into tokens using TreebankWordTokenizer().tokenize(),\n",
    "    lowercase each token\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        a path to the text file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens : list of strings\n",
    "        text as a list of lowercased tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    for sentence in sentences:\n",
    "        words = tokenizer.tokenize(sentence)\n",
    "        tokens.extend([word.lower() for word in words]) \n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_bug = tokenize_and_lowercase(bug_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29091621351f46ef40a80087fa5c308d",
     "grade": true,
     "grade_id": "cell-277001137d76c0ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if the number of tokens is correct\n",
    "assert_equal(len(tokenize_and_lowercase(bug_file_path)), 16172)\n",
    "# checks if all tokens are lowercased\n",
    "assert_equal(all([x.lower for x in tokenize_and_lowercase(bug_file_path)]), True)\n",
    "# checks if the first token is correct\n",
    "assert_equal(tokenize_and_lowercase(bug_file_path)[0], 'the')\n",
    "# checks if the last 5 tokens are correct\n",
    "assert_equal(tokenize_and_lowercase(bug_file_path)[-5:], ['who', 'shall', 'tell', '?', '”'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81ec67d36c303cbe8d9af2a90e6e8297",
     "grade": false,
     "grade_id": "cell-c5b82c31e7e96a54",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 3 <a class=\"anchor\" id=\"task_3\"></a>\n",
    "## Word frequencies\n",
    "\n",
    "In this task we will explore the distribution of word frequencies and discuss what influence it can have on different NLP tasks. Here you need to remember the definitions of word token and word type given at the beginning of [task 2](#task_2).\n",
    "\n",
    "## 3.1 <a class=\"anchor\" id=\"subtask_3_1\"></a>\n",
    "### Analyse word frequencies (3 points)\n",
    "\n",
    "You've already recorded the statistics of letters and letter pairs, now you can repurpose those functions to answer the following questions:\n",
    "\n",
    "1. How many word tokens are there in the text? (0.6 points)\n",
    "2. How many word types are there in the text? (0.6 points)\n",
    "3. What are 10 most frequent word tokens? Report them as a list starting with the most frequent one? (0.6 points)\n",
    "4. What is the fraction of word types (out of all word types) that appeared in the text only 2 times or less? (0.6 points)\n",
    "5. What is the fraction of word types (out of all word types) that appeared in the text 50 times or more? (0.6 points)\n",
    "\n",
    "Type your answers in the cell below. You can create an additional cell to do the calculations if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "683ce22c40d569de79a5b6ee87ee4f52",
     "grade": false,
     "grade_id": "cell-a8f655518f58b3fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Word Tokens: 16172\n",
      "Number of Word Types: 2977\n",
      "10 Most Frequent Word Tokens: [',', 'the', '.', 'of', 'and', 'to', 'a', 'i', 'in', 'it']\n",
      "Fraction of Word Types Appearing 2 Times or Less: 0.7467248908296943\n",
      "Fraction of Word Types Appearing 50 Times or More: 0.014779979845482029\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tokens = tokenize_and_lowercase(bug_file_path)\n",
    "\n",
    "n_word_tokens = len(tokens)\n",
    "print(\"Number of Word Tokens:\", n_word_tokens)\n",
    "\n",
    "word_types = set(tokens)\n",
    "n_word_types = len(word_types)\n",
    "print(\"Number of Word Types:\", n_word_types)\n",
    "\n",
    "counter = Counter(tokens)\n",
    "top_ten_words = [word for word, count in counter.most_common(10)]\n",
    "print(\"10 Most Frequent Word Tokens:\", top_ten_words)\n",
    "\n",
    "freq_2_or_less = sum(1 for word in word_types if counter[word] <= 2)\n",
    "frac_2_or_less = freq_2_or_less / n_word_types\n",
    "print(\"Fraction of Word Types Appearing 2 Times or Less:\", frac_2_or_less)\n",
    "\n",
    "freq_50_or_more = sum(1 for word in word_types if counter[word] >= 50)\n",
    "frac_50_or_more = freq_50_or_more / n_word_types\n",
    "print(\"Fraction of Word Types Appearing 50 Times or More:\", frac_50_or_more)\n",
    "\n",
    "\n",
    "# put your answer to question 1 as an int into the variable below\n",
    "# For example:\n",
    "# n_word_tokens = 1000\n",
    "n_word_tokens = 16172 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 3 as an int into the variable below\n",
    "# For example:\n",
    "# n_word_types = 1000\n",
    "n_word_types = 2977 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 3 as list of strings into the variable below\n",
    "# For example:\n",
    "# top_ten_words = ['hello', 'world']\n",
    "top_ten_words = [',', 'the', '.', 'of', 'and', 'to', 'a', 'i', 'in', 'it'] ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 4 as a float into the variable below\n",
    "# For example:\n",
    "# frac_2_or_less = 0.12345\n",
    "frac_2_or_less = 0.7467248908296943 ##FILL IN THE ANSWER\n",
    "\n",
    "# put your answer to question 5 as a float into the variable below\n",
    "# For example:\n",
    "# frac_50_or_more = 0.12345\n",
    "frac_50_or_more = 0.014779979845482029 ##FILL IN THE ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f062ec7e940a09f598e84d5c38dc5e8c",
     "grade": true,
     "grade_id": "cell-727ab27821e90861",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_word_tokens),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c3cdbbac4acf48b6eab871a7527b12a",
     "grade": true,
     "grade_id": "cell-f40a9b7de3dfe7cb",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is an int\n",
    "assert_equal(type(n_word_types),int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4374030f37a95c4ce5441f98a7b0df0d",
     "grade": true,
     "grade_id": "cell-e2c84eb3451e0d60",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is of the right formt\n",
    "assert_equal(type(top_ten_words),list)\n",
    "assert_equal(type(top_ten_words[0]),str)\n",
    "assert_equal(len(top_ten_words),10)\n",
    "# checks if you've got the most frequent token right\n",
    "assert_equal(top_ten_words[0],',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "671063d31b65e77c4d0931e1560a4230",
     "grade": true,
     "grade_id": "cell-9d91ee0fe145007a",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_2_or_less),float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "168fd92376e9b2ab651963c44b87399d",
     "grade": true,
     "grade_id": "cell-b8de5c57ecf06fd4",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if your answer is a float\n",
    "assert_equal(type(frac_50_or_more),float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46db60d11c8e2f05c0b4539d94509726",
     "grade": false,
     "grade_id": "cell-fb0a4f7abc9b2f63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.2 <a class=\"anchor\" id=\"subtask_3_2\"></a>\n",
    "### Remove stop words (1 point)\n",
    "As you can see, the most frequent word types are not specific to the Poe's story, but are pretty much the same across English language.\n",
    "\n",
    "In information theory, the more likely an event is to occur, the less information it contains. Thus, if an event is not a surprise, it's simply \"old news\". For some natural language applications, it means that words like *to* and *the* don't tell anything important about a text. They are not helpful in recognising its topic or its author, for instance.\n",
    "\n",
    "Such frequent uninformative words are called **stop words**, and, in some cases, they can simply be cleaned out from data. There exist prepared lists of such words in English. Let's remove stop words using a list provided by the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "deb653f66e89160a80ea3e1a2862e1d5",
     "grade": false,
     "grade_id": "cell-fb4b2ec2f039e056",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(tokenized_text, stop_words):\n",
    "    \"\"\"This function removes stop words from lowercased tokenized text\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_text : list of strings\n",
    "        lowercased text tokens \n",
    "    stop_words : list of strings\n",
    "        a list of words to remove\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clean_text : list of strings\n",
    "        list of text tokens with stop words removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    clean_text = [word for word in tokenized_text if word not in stop_words]\n",
    "    \n",
    "    return clean_text\n",
    "    \n",
    "clean_bug = remove_stop_words(tokenized_bug, stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "923767a02abcb304ff6a99ea33695038",
     "grade": true,
     "grade_id": "cell-09346ef37946973d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checks if the number of tokens is right\n",
    "assert_equal(len(clean_bug), 9309)\n",
    "\n",
    "# checks if the first token is right\n",
    "assert_equal(clean_bug[0], 'gold-bug')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ec3c43ad259c5305ab84e2992e8cdea",
     "grade": false,
     "grade_id": "cell-0e07aeb2833401f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "(Click the **Validate** button in the upper menu to check that you haven't missed anything.) **NB** At the moment, the validate button doesn't work on the upper menu. **Instead, you can validate the notebook in the Assignments tab.**\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** tab, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=689919) section of Mycoures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
