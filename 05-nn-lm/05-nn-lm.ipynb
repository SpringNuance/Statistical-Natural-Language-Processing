{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b256b1ddfe4fd92c2014bbb23fd65ee",
     "grade": false,
     "grade_id": "cell-846d50d8b492aac9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 7: Neural language models\n",
    "\n",
    "# Released: 13.2.2024\n",
    "# Deadline: 23.2.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nose==1.3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d32313c7b30bf29fffdfda5624934b5e",
     "grade": false,
     "grade_id": "cell-c21f8a399f3e7cc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "After completing this exercise, you will understand how the neural language models work.\n",
    "Additionally, you will learn how to construct one and generate sentences with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1201e9774fd45e6c70343fd4221ebb06",
     "grade": false,
     "grade_id": "cell-508b79519d79dccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Introduction](#intro)\n",
    "    * [Language models](#language_models)\n",
    "    * [Neural language models](#neural_lm)\n",
    "* [Task 1: Data preprocessing](#task_1)\n",
    "    * [Index dictionaries](#task_1_1)\n",
    "    * [Index data](#task_1_2)\n",
    "    * [Prepare features and labels](#task_1_3)\n",
    "    * [Preprocess data](#task_1_4)\n",
    "* [Task 2: FFNN language model](#task_2)\n",
    "    * [Create the FFNN model](#task_2_1)\n",
    "    * [Generate text](#task_2_2)\n",
    "* [Task 3: RNN language model](#task_3)\n",
    "    * [Create the RNN model](#task_3_1)\n",
    "    * [Generate text](#task_3_2)\n",
    "* [Task 4: Output analysis](#task_4)\n",
    "    * [Compare the models](#task_4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed2e5919d8322e5c1d6c6f0f59215201",
     "grade": false,
     "grade_id": "cell-4891ef30b009b405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "## Language models <a class=\"anchor\" id=\"language_models\"></a>\n",
    "From the n-grams exercise, you are already familiar with what language models are. As a recap, the goal of the language model is to predict the next word that is going to appear, based on the previous words that appeared.\n",
    "$P(w_i| w_{iâˆ’1} . . . w_0)$.\n",
    "\n",
    "## Neural language models <a class=\"anchor\" id=\"neural_lm\"></a>\n",
    "The n-gram language models have some shortcomings. They generally can't capture long-span dependencies. Additionally, they are dependant on the word order. These limitations are mitigated by the neural network language models.\n",
    "\n",
    "The neural network language models use word embeddings, instead of the words themselves. The word embeddings are a distributed representation of the words and they have a nice property that similar words are close in the vector space. This solves the word order dependency problem that the n-gram language models have.\n",
    "\n",
    "The recurrent neural networks (RNN) process the input sequentially, where each next prediction is influenced by the history. This allows them to capture long-term dependencies between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5acda9481b601d8386d017168df5c94",
     "grade": false,
     "grade_id": "cell-dc9406d5e7b087e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1: Data preprocessing <a class=\"anchor\" id=\"task_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b1b735d730b457ea5ac36773211d3ef",
     "grade": false,
     "grade_id": "cell-1122e41040dee431",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Importing the dependencies\n",
    "\n",
    "The first thing that we need to do is to import the necessary dependencies. To create the neural language models, we are going to use the [Pytorch](https://pytorch.org/) deep learning framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95bbcc0d40f3c65684d3beaab7740c7f",
     "grade": false,
     "grade_id": "cell-8e5f47d16bada134",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f75101cb630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading pytorch\n",
    "import torch\n",
    "import torch.nn as nn # the neural network package that contains functions for creating the neural network layers\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # a package that allows use to use an optimizer in order to update the parameters during training\n",
    "from torch.utils.data import DataLoader # allows use to process the data in batches\n",
    "from torch.nn.utils.rnn import pad_sequence # a function that zero-pads the sentences so they can have equal size in a batch\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torch.manual_seed(0) # set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27c1acab14046fe460dbba91609206b7",
     "grade": false,
     "grade_id": "cell-66ce509efb99899c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load the data\n",
    "\n",
    "In this assignment, as data, we are going to use the \"Pride and Prejudice\", same as in the n-grams assignment. The data can be obtained from [here](https://www.gutenberg.org/ebooks/1342).\n",
    "\n",
    "The cell below loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "361ddcbfce8238e8420d912b2f3cfb5a",
     "grade": false,
     "grade_id": "cell-89129c05a6f5df8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../coursedata/nn-lm/janeausten.txt', 'r') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73a507f05244b7a4ff607d979451eb4c",
     "grade": false,
     "grade_id": "cell-ec253c695def93dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Sentence boundaries\n",
    "\n",
    "When dealing with language, it is good to know when a sentence starts and when it ends. That will help the model at the beginning of the prediction, when we don't have any previous words as context. For that purpose, we are going to pad each sentence with a start-of-sentence symbol _\"&lt;s>\"_ and an end-of-sentence symbol _\"&lt;/s>\"_. \n",
    "\n",
    "Since you already did a similar thing in the n-grams exercise, this function is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7c0e966f3e8ed94f1b99e29ed1b22f6",
     "grade": false,
     "grade_id": "cell-8dc8b3d84c549228",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_sentence_boundaries(data):\n",
    "    \"\"\"\n",
    "    Takes the data, where each line is a sentence, appends <s> token at the beginning and </s> at the end of each sentence\n",
    "    Example input: I live in Helsinki\n",
    "    Example output: <s> I live in Helsinki </s>\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data : list\n",
    "            a list of sentences\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res : list\n",
    "            a list of sentences, where each sentence has <s> at the beginning and </s> at the end\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sent in data:\n",
    "        sent = '<s> ' + sent.rstrip() + ' </s>'\n",
    "        res.append(sent)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f0fae9e88233c4d48ec5b0c1e993d19",
     "grade": false,
     "grade_id": "cell-8bd9afc6b3df34ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1  Index dictionaries <a class=\"anchor\" id=\"task_1_1\"></a> (1 Point)\n",
    "Neural networks can't process words as raw strings. Due to that, we need to represent the words with numbers. The first step in doing that is creating two dictionaries: word2idx and idx2word.\n",
    "\n",
    "The word2idx dictionary contains unique words as keys and unique indices for each of the words as values. <br>\n",
    "The idx2word dictionary contains unique indices as keys and unique words for each of those indices as values. It is essentially a reversed word2dx, where the keys are the values and the values are the keys.\n",
    "\n",
    "Example sentences: [\"I look forward\", \"You look forward\"] <br>\n",
    "word2idx = {\"I\": 1, \"look\": 2, \"forward\": 3, \"You\": 4} <br>\n",
    "idx2word = {1: \"I\", 2: \"look\", 3: \"forward\", 4: \"You\"} <br>\n",
    "\n",
    "Write a function that creates two dictionaries: word2idx and idx2work. The dictionaries should contain all the unique words in the data. <b>The indices should start from 1 and not from 0</b>\n",
    "    \n",
    "<b>Additionally, the first index should correspond to the first word in the `data` variable, second word to the second, etc.</b>\n",
    "\n",
    "We need the indices in a specific order because the pre-trained models are trained with indices in that particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ae92f327b03e74833eefe5d6159a722",
     "grade": false,
     "grade_id": "cell-29eb8484bba4f325",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_indices(data):\n",
    "    \"\"\"\n",
    "    This function creates two dictionaries: word2idx and idx2word, containing each unique word in the dataset\n",
    "    and its corresponding index.\n",
    "    Remember that the starting index should be 1 and not 0\n",
    "    Remember that the first word in 'data' should coeerespond to the first index\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    index = 1  # Starting index\n",
    "    \n",
    "    # Splitting each sentence into words and updating the dictionaries\n",
    "    for sentence in data:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = index\n",
    "                idx2word[index] = word\n",
    "                index += 1\n",
    "                \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5ad853e20476faec8bc72962375c4bf",
     "grade": true,
     "grade_id": "cell-b97715fd686184a6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_data = ['<s> a girl likes eating by herself', '</s> the cat likes eating fish']\n",
    "word2idx_dummy, idx2word_dummy = create_indices(dummy_data)\n",
    "\n",
    "# check if the results returned are dictionaries\n",
    "assert_equal(type(word2idx_dummy), dict)\n",
    "assert_equal(type(idx2word_dummy), dict)\n",
    "\n",
    "# check if word2idx and idx2word are the same length\n",
    "assert_equal(len(word2idx_dummy), len(idx2word_dummy))\n",
    "\n",
    "# check if all the unique words in the data set have indices\n",
    "combined_dummy_data = dummy_data[0] + ' ' + dummy_data[1]\n",
    "combined_dummy_data = combined_dummy_data.split()\n",
    "assert_equal(len(set(combined_dummy_data)), len(idx2word_dummy))\n",
    "\n",
    "# check if values are integers in word2idx\n",
    "assert_equal(type(word2idx_dummy['girl']), int)\n",
    "assert_equal(type(word2idx_dummy['fish']), int)\n",
    "assert_equal(type(word2idx_dummy['<s>']), int)\n",
    "\n",
    "# check if values are strings in idx2word\n",
    "assert_equal(type(idx2word_dummy[1]), str)\n",
    "assert_equal(type(idx2word_dummy[2]), str)\n",
    "assert_equal(type(idx2word_dummy[3]), str)\n",
    "\n",
    "# check if word2idx and idx2word have the same values\n",
    "dummy_idx = word2idx_dummy['<s>']\n",
    "dummy_word = idx2word_dummy[dummy_idx]\n",
    "assert_equal(word2idx_dummy[dummy_word], dummy_idx)\n",
    "assert_equal(idx2word_dummy[dummy_idx], dummy_word)\n",
    "\n",
    "# check if the first value in idx2word is 1\n",
    "assert_equal(list(idx2word_dummy.keys())[0], 1)\n",
    "\n",
    "# check if the first word in the dataset corresponds to the first index, second word to the second index, etc\n",
    "assert_equal(word2idx_dummy['<s>'], 1)\n",
    "assert_equal(word2idx_dummy['a'], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9666c598ce5f82301664682ef22c1b7",
     "grade": false,
     "grade_id": "cell-10d3120044cbf2a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2  Index data <a class=\"anchor\" id=\"task_1_2\"></a> (1 Point)\n",
    "After we have created the word2idx and idx2word dictionaries, it is time to index the data. In other words, we need to replace each word in the data with its corresponding index.\n",
    "\n",
    "Write a function that reads each sentence from the data and replaces each word in the sentence with its index from the word2idx dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eeec6eccd55854d96457df39686fa76",
     "grade": false,
     "grade_id": "cell-bcd6e6181f13b4ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_data(data, word2idx):\n",
    "    \"\"\"\n",
    "    This function replaces each word in the data with its corresponding index\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences, where each sentence starts with <s>\n",
    "            and ends with </s> token\n",
    "    \n",
    "    word2idx - dict\n",
    "            a dictionary where the keys are the unique words in the data\n",
    "            and the values are the unique indices corresponding to the words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_indexed - list\n",
    "                a list of sentences, where each word in the sentence is replaced with its index\n",
    "    \"\"\"\n",
    "    \n",
    "    data_indexed = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    for sentence in data:\n",
    "        indexed_sentence = [word2idx[word] for word in sentence.split()]\n",
    "        data_indexed.append(indexed_sentence)\n",
    "        \n",
    "    return data_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "960934e6efe1cbe0d2c126130060b3d1",
     "grade": true,
     "grade_id": "cell-61e868b4dbda9787",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_data = ['<s> a girl is here </s>', '<s> a boy is there </s>']\n",
    "dummy_word2idx = {'<s>': 1, 'a': 2, 'girl': 3, 'is': 4, 'here': 5, '</s>': 6, 'boy': 7, 'there': 8}\n",
    "dummy_indexed_data = index_data(dummy_data, dummy_word2idx)\n",
    "\n",
    "# check that the returned result is a list\n",
    "assert_equal(type(dummy_indexed_data), list)\n",
    "\n",
    "# check that the length of the results is the same as the length of the data\n",
    "assert_equal(len(dummy_data), len(dummy_indexed_data))\n",
    "\n",
    "# check that the function does what it is supposed to do\n",
    "assert_equal(dummy_indexed_data[0], [1, 2, 3, 4, 5, 6])\n",
    "assert_equal(dummy_indexed_data[1], [1, 2, 7, 4, 8, 6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7db069593c019785d1f4c86f0460b5b5",
     "grade": false,
     "grade_id": "cell-e9cef6f6c95f9ab1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Convert sentences to tensors\n",
    "\n",
    "This function converts each indexed sentence to a LongTensor data type. This is required in order to process it later using Pytorch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4285322821031ec9d36190a0ddcd739f",
     "grade": false,
     "grade_id": "cell-e1b1dc71c6baec5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_tensor(data_indexed):\n",
    "    \"\"\"\n",
    "    This function converts the indexed sentences to LongTensors\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor_array - list\n",
    "                a list of sentences, where each sentence\n",
    "                is a LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor_array = []\n",
    "    for sent in data_indexed:\n",
    "        tensor_array.append(torch.LongTensor(sent))    \n",
    "        \n",
    "    return tensor_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7ba4727d8cd526d48f603680ca4088d",
     "grade": false,
     "grade_id": "cell-5e962e4653d12e29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Combine features and labels in a tuple\n",
    "\n",
    "This function combines each indexed sentence and its corresponding labels to a tuple. This will be beneficial for us when we zero-pad the data later, in order to make the batches have equal-length samples.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eda5a06e811e735148032533b6c740a",
     "grade": false,
     "grade_id": "cell-52d9900ae43cd3bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_data(input_data, labels_data):\n",
    "    \"\"\"\n",
    "    This function converts the input features and the labels into tuples\n",
    "    where each tuple corresponds to one sentence in the format (features, labels)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    input_data - list\n",
    "            a list of tensors containing the training features\n",
    "    \n",
    "    labels_data - list\n",
    "            a list of tensors containing the training labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res - list\n",
    "            a list of tuples, where each tuple corresponds to one sentece pair\n",
    "            in the format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        res.append((input_data[i], labels_data[i]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6315af90e75144392a59629127d9e14b",
     "grade": false,
     "grade_id": "cell-ddb361d733a03a19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Remove extra data\n",
    "\n",
    "Since we will be processing the data in equal batches during training, we need to make sure that each batch has equal number of sentences. In case the last batch contains less sentences than the batch size, that batch will be discarded.\n",
    "\n",
    "This function discards the extra data that doesn't fit in a batch.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f978891c2828fc680f48f186d33b51f3",
     "grade": false,
     "grade_id": "cell-a6d729f55cdbbffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_extra(data, batch_size):\n",
    "    \"\"\"\n",
    "    This function removes the extra data that does not fit in a batch   \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "            \n",
    "    batch_size - integer\n",
    "                    the size of the batch\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data - list\n",
    "            a list of tuples, where each tuple corresponds to a\n",
    "            sentence in a format (features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    extra = len(data) % batch_size\n",
    "    if extra != 0:\n",
    "        data = data[:-extra][:]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51d6ed0fed32982965a755de14ba3bdc",
     "grade": false,
     "grade_id": "cell-e6b666a51f6a84fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zero-pad the data\n",
    "\n",
    "In order to process the data in batches, we need to make sure that the sentences in each batch have equal lengths. Since we are working with sentences, each sentence in a batch can have different number of words. In this case, we need to  make the length of each sentence the same as the length of the longest sentence in that batch. We do that by adding zeros at the end of each sentence, until the sentence has equal length as the longest one in the batch.\n",
    "\n",
    "This function implements the zero-padding.\n",
    "\n",
    "You don't have to modify this function. It is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2808c5a200ce80f3cfb0246ed390d56",
     "grade": false,
     "grade_id": "cell-e5f21077b5ea9e3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate(list_of_samples):\n",
    "    \"\"\"\n",
    "    This function zero-pads the training data in order to process the sentences\n",
    "    in a batch during training\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    list_of_samples - list\n",
    "                        a list of tuples, where each tuple corresponds to a\n",
    "                        sentence in a format (features, labels)\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pad_input_data - tensor\n",
    "                        a tensor of input features equal to the batch size,\n",
    "                        where features are zero-padded to have equal lengths\n",
    "                        \n",
    "    input_data_lengths - list\n",
    "                        a list where each element is the length of the \n",
    "                        corresponding sentence\n",
    "    \n",
    "    pad_labels_data - tensor\n",
    "                        a tensor of labels equal to the batch size,\n",
    "                        where labels are zero-padded to have equal lengths\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    list_of_samples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_data, labels_data = zip(*list_of_samples)\n",
    "\n",
    "    input_data_lengths = [len(seq) for seq in input_data]\n",
    "    \n",
    "    padding_value = 0\n",
    "\n",
    "    # pad input\n",
    "    pad_input_data = pad_sequence(input_data, padding_value=padding_value)\n",
    "    \n",
    "    # pad labels\n",
    "    pad_labels_data = pad_sequence(labels_data, padding_value=padding_value)\n",
    "\n",
    "    return pad_input_data, input_data_lengths, pad_labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1a57689ab475c45103897bf3a6ce1f1",
     "grade": false,
     "grade_id": "cell-1a9a59d43a7d0704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Prepare features and labels <a class=\"anchor\" id=\"task_1_3\"></a> (1 Point)\n",
    "During training, the model takes an input word and outputs a prediction. We will need to compare this prediction to 'true label'. True label is just the next word in the text, but we will need to organize the data, so that every word in the text is considered as this 'true label'.\n",
    "\n",
    "In the label sentence, every word is moved a step in time, and for the input sentence the last word is missing. \n",
    "\n",
    "Example sentence: oops i did it again <br>\n",
    "INPUT: oops i did it <br>\n",
    "LABEL: i did it again\n",
    "\n",
    "Note: the first word in the sentence is start-of-sentence symbol and the last one is end-of-sentence symbol.\n",
    "\n",
    "Write a function that takes as input the indexed data and returns two arrays: the input array where the last word from each sentence is missing, and the label array, where every word is moved a step in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2ed84a322abc664c52dca43409a2c08",
     "grade": false,
     "grade_id": "cell-04fd2b4bad041241",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(data_indexed):\n",
    "    \"\"\"\n",
    "    This function creates the input features and their corresponding labels\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data_indexed - list\n",
    "            a list of sentences, where each word in the sentence\n",
    "            is replaced by its index\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of indexed sentences, where the last element of each sentence is removed\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of indexed sentences, where the first element of each sentence is removed\n",
    "    \"\"\"\n",
    "    \n",
    "    input_data = []\n",
    "    labels_data = []\n",
    "\n",
    "     # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    for sentence in data_indexed:\n",
    "        input_data.append(sentence[:-1])\n",
    "        labels_data.append(sentence[1:])\n",
    "        \n",
    "    return input_data, labels_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55e9468b6e82c273ae9649be58faaaa7",
     "grade": true,
     "grade_id": "cell-f5065279195ee97f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_data = [[1, 2, 3, 4, 5, 6], [4, 6, 2, 6, 7]]\n",
    "dummy_train_input, dummy_train_labels = prepare_for_training(dummy_data)\n",
    "\n",
    "# check that the returned results are lists\n",
    "assert_equal(type(dummy_train_input), list)\n",
    "assert_equal(type(dummy_train_labels), list)\n",
    "\n",
    "# check that the length of the input and the labels match\n",
    "assert_equal(len(dummy_train_input), len(dummy_train_labels))\n",
    "assert_equal(len(dummy_train_input[0]), len(dummy_train_labels[0]))\n",
    "assert_equal(len(dummy_train_input[1]), len(dummy_train_labels[1]))\n",
    "\n",
    "# check that the function works as it should\n",
    "assert_equal(dummy_train_input[0], [1, 2, 3, 4, 5])\n",
    "assert_equal(dummy_train_input[1], [4, 6, 2, 6])\n",
    "\n",
    "assert_equal(dummy_train_labels[0], [2, 3, 4, 5, 6])\n",
    "assert_equal(dummy_train_labels[1], [6, 2, 6, 7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af404661b0991e6e0c0f0cbf64213a52",
     "grade": false,
     "grade_id": "cell-bdd4fc1b5167b75c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Preprocess data <a class=\"anchor\" id=\"task_1_4\"></a> (1 Point)\n",
    "At this point, we have all the necessary functions to prepare the data for training. What is left to do is to run them one by one and get the data in the desired format.\n",
    "\n",
    "Write a function that takes the data and prepares it for training. You need to do the following steps:\n",
    "\n",
    "    1. Add sentence boundaries\n",
    "    2. Create index dictionaries (word2idx and idx2word)\n",
    "    3. Index the data in a way that each word is replaced by its index\n",
    "    4. Convert the indexed data to a list of tensors, where each tensor is a sentence\n",
    "    5. Split each sentence to input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fae403026e8848f39a37c9c9f148bd49",
     "grade": false,
     "grade_id": "cell-605424fd32ecf277",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    This function runs the whole preprocessing pipeline and returns the prepared\n",
    "    input features and labels, along with the word2idx and idx2word dictionaries\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    data - list\n",
    "            a list of sentences that need to be prepared for training\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    input_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as input feature\n",
    "            \n",
    "    labels_data - list\n",
    "            a list of tensors, where each tensor is an indexed sentence used as a true label\n",
    "    \n",
    "    word2idx - dictionary\n",
    "                a dictionary, where the keys are the words and the values are the indices\n",
    "                \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    data_with_boundaries = add_sentence_boundaries(data)\n",
    "\n",
    "    unique_words = set(\" \".join(data_with_boundaries).split())\n",
    "    word2idx = {word: idx for idx, word in enumerate(unique_words, start=1)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    data_indexed = index_data(data_with_boundaries, word2idx)\n",
    "    input_data, labels_data = prepare_for_training(data_indexed)\n",
    "    \n",
    "    return input_data, labels_data, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7271bed1a9e83e10f6f5131d189de1a",
     "grade": true,
     "grade_id": "cell-978955c72672b86a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_data = ['a girl likes eating by herself', 'the cat likes eating fish']\n",
    "dummy_input, dummy_labels, dummy_word2idx, dummy_idx2word = preprocess_data(dummy_data)\n",
    "\n",
    "# check that the returned results are lists\n",
    "assert_equal(type(dummy_input), list)\n",
    "assert_equal(type(dummy_labels), list)\n",
    "\n",
    "# check that the returned results have the same lengths\n",
    "assert_equal(len(dummy_input), len(dummy_labels))\n",
    "\n",
    "# check that the sizes of the indexed sentences are correct\n",
    "assert_equal(len(dummy_input[0]), len(dummy_data[0].split()) + 1)\n",
    "assert_equal(len(dummy_input[1]), len(dummy_data[1].split()) + 1)\n",
    "\n",
    "assert_equal(len(dummy_labels[0]), len(dummy_data[0].split()) + 1)\n",
    "assert_equal(len(dummy_labels[1]), len(dummy_data[1].split()) + 1)\n",
    "\n",
    "# check that the input features and labels are correct\n",
    "assert_equal(dummy_input[0][1], dummy_labels[0][0])\n",
    "assert_equal(dummy_input[0][2], dummy_labels[0][1])\n",
    "assert_equal(dummy_input[0][3], dummy_labels[0][2])\n",
    "assert_equal(dummy_input[0][4], dummy_labels[0][3])\n",
    "assert_equal(dummy_input[0][5], dummy_labels[0][4])\n",
    "\n",
    "assert_equal(dummy_input[1][1], dummy_labels[1][0])\n",
    "assert_equal(dummy_input[1][2], dummy_labels[1][1])\n",
    "assert_equal(dummy_input[1][3], dummy_labels[1][2])\n",
    "assert_equal(dummy_input[1][4], dummy_labels[1][3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "075954bc841aaae04edf5f392e13702b",
     "grade": false,
     "grade_id": "cell-8befe0e3d249e5fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we are going to call the preprocessing function and obtain the features and labels. After that, we will combine the features and labels in tuples using the `combine_data` function and then remove the extra data that does not fit in a batch using the `remove_extra` function. At the end, we are going to call the `DataLoader`, which prepares the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e504876e69cf430ba09d76f2525df33a",
     "grade": false,
     "grade_id": "cell-04e840e1825cbbf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input, train_labels, word2idx, idx2word = preprocess_data(data) # run the preprocessing pipeline\n",
    "\n",
    "batch_size = 16 # the number of sentences to be processed at once\n",
    "\n",
    "train_data = combine_data(train_input, train_labels)\n",
    "train_data = remove_extra(train_data, batch_size)\n",
    "\n",
    "pairs_batch_train = DataLoader(dataset=train_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70f803359c06f00b004853e7bd5455b5",
     "grade": false,
     "grade_id": "cell-1bd781580daacb3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2:  Feed forward neural network language model <a class=\"anchor\" id=\"task_2\"></a>\n",
    "In this task, we are going to implement our first neural network language model. To do that, we are going to use a feed-forward neural network that takes the previous word as input and predicts the next word based on it. This is similar to the bigram language model.\n",
    "\n",
    "To predict the next word, we first need to convert the previous word into a vecor $x(t)$. In other words, we need to embed it. <br>\n",
    "Next, we need to apply a linear transformation $ h(t) = Ax(t) + b $ to compute a representation of linear distributional features. In this case $ A $ is a learnable matrix and $ b $ is the bias, which is also a learnable parameter. <br>\n",
    "In the previous step we applied a linear transformation. In order for the model to learn more complex feature representations, we can add a non-linearity after the linear transformation: $ U(h(t)) $, which in our case is a [Rectified Linear Unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU). <br>\n",
    "One common way of preventing the model from overfitting is to apply a dropout. The dropout disconnects random neurons witch a certain probability, preventing the network from overlearning the training set. <br>\n",
    "Since we need to predict the next word, we need the output of the transformation to be equal to the number of unique words in the data. To do that, we need to apply another linear transformation, which has an output size same as the number of unique words in the data. <br>\n",
    "In the end, the output is passed to the CrossEntropy loss function that estimates how good is the model's prediction in comparison to the true labels.\n",
    "\n",
    "Generally, with deep learning frameworks, like Pytorch, we only need to implement the computations from the input to the output. This is also called a \"forward pass\" and is the `forward` function in the `FFNN` class. That in our case would be passing the previous word and getting a probability distribution over the next word.\n",
    "\n",
    "On the other hand, the calculation of the partial derivatives of the loss function are calculated automatically by the framework and we don't need to implement it. This is also called a \"backward pass\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0748b1dc70a476a4cd454e729c725765",
     "grade": false,
     "grade_id": "cell-26bb204d783d8381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Create the model (3 Points) <a class=\"anchor\" id=\"task_2_1\"></a>\n",
    "\n",
    "The `FFNN` class contains the definition of the model that we are going to train for the language modeling task. The first function that the class has is called `__init__` and it initializes the layers of the model. This function is already implemented for you.\n",
    "\n",
    "The next function is called `forward` and is the forward pass explained earlier. You need to implement this function.\n",
    "\n",
    "To implement the function, you need to perform the following steps:\n",
    "\n",
    "    1. Replace the indexed word with its embedding vector. In other words, pass it through the embedding layer\n",
    "    2. Apply the first linear transformation to the embedding of the word. In other words, pass the word embedding through the linear layer, defined as `self.lin = nn.Linear(self.embed_dim, self.context_dim)`\n",
    "    3. Apply the ReLU non-linearity to the output of the linear projection defined in the previous step. The ReLU function can be called as `F.relu()`\n",
    "    4. Apply a dropout after ReLU\n",
    "    5. Apply the second linear transformation to the output after the dropout. The second linear transformation is defined as `self.out = nn.Linear(self.context_dim, len(self.word2idx)+1)`\n",
    "    6. Return the output of the second linear transformation\n",
    "    \n",
    "Remember that now we are processing the words in batches. If we have a batch size of 5, that means that we are passing 5 words at a time and applying all the transformations to those 5 words simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f044204e00968554fc6a773e66178d29",
     "grade": false,
     "grade_id": "cell-040e0f63d5ed2073",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, word2idx, embed_dim, context_dim):\n",
    "        \"\"\"\n",
    "        This function initializes the layers of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "        \n",
    "        embed_dim - integer\n",
    "                        the size of the word embeddings\n",
    "\n",
    "        context_dim - integer\n",
    "                        the dimension of the hidden size\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FFNN, self).__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_dim = context_dim\n",
    "        \n",
    "        # here we initialise the layers of the model\n",
    "        self.word_embed = nn.Embedding(len(self.word2idx)+1, self.embed_dim) # embedding layer\n",
    "    \n",
    "        self.lin = nn.Linear(self.embed_dim, self.context_dim) # linear layer\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1) # dropout layer\n",
    "        \n",
    "        self.out = nn.Linear(self.context_dim, len(self.word2idx)+1) # output layer\n",
    "        \n",
    "    \n",
    "    def forward(self, word):\n",
    "        \"\"\"\n",
    "        This function implements the forward pass of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word - tensor\n",
    "                    a tensor containing indices of the words in a batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output - tensor\n",
    "                    a tensor of logits from the second linear transformation\n",
    "        \"\"\" \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "        embedded_words = self.word_embed(word)\n",
    "        linear_output = self.lin(embedded_words)\n",
    "        relu_output = F.relu(linear_output)\n",
    "        dropout_output = self.dropout(relu_output)\n",
    "        output = self.out(dropout_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63e60c15d925ad0ca10841b7e42e98ec",
     "grade": true,
     "grade_id": "cell-98d36edab2d74493",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_ff_model = FFNN(word2idx, 10, 20)\n",
    "dummy_train_input = torch.randint(1, 10, (2,))\n",
    "dummy_output = dummy_ff_model(dummy_train_input)\n",
    "\n",
    "# test that the returned result is a tensor\n",
    "assert_equal(torch.is_tensor(dummy_output), True)\n",
    "\n",
    "# test that the shapes match\n",
    "assert_equal(dummy_output.size(), (2, len(word2idx)+1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ada6555d9f269df41c4b9632e6f66b",
     "grade": false,
     "grade_id": "cell-26003b24ed187c75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Model initialization\n",
    "In the next cell, we are going to define the hyperparameters of our neural network language model. Additionally, we will initialize the model, along with the loss function and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f75bdda82a724b24be8f5c3ce448fcb2",
     "grade": false,
     "grade_id": "cell-5e10e6e978b35159",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 30 # the number of epochs to train\n",
    "embedding_size = 300 # the size of the embedding layer\n",
    "hidden_size = 450 # the size of the linear projection\n",
    "ff_model = FFNN(word2idx, embedding_size, hidden_size) # initialize the model\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0) # the loss function which compares how good the NN is predicting the next word\n",
    "ffnn_optimizer = optim.Adam(ff_model.parameters(), lr=0.005) # Adam optimizer for updating the parameters during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f09fe94f4c90e871e96b1776e04ef43",
     "grade": false,
     "grade_id": "cell-5c171a1594354fd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training the model\n",
    "Now that we have initialized the model, the next step is to train it. The training process is done by passing the previous word through the forward pass. Then, the loss function compares how good the neural network is predicting the next word. Then we run the backward pass of the network where the partial derivatives of the loss function are computed. At the end, based on those partial derivatives, the parameters of the network get updated using the Adam optimizer. The process is repeated for $n$ number of epochs, where one epoch passes when we process all the sentences in the training set.\n",
    "\n",
    "You don't need to modify this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5600bd0b61352a11fa527dfebf5c6308",
     "grade": false,
     "grade_id": "cell-9aa00560847d8739",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(pairs_batch_train, ff_model, loss_function, ffnn_optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    This function implements the training of the model\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs_batch_train - object\n",
    "                            a DataLoader object that contains the batched data\n",
    "\n",
    "    ff_model - object\n",
    "                a FFNN object that contains the initialized model\n",
    "\n",
    "    loss_function - object\n",
    "                        the CrossEntropy loss function\n",
    "\n",
    "    ffnn_optimizer - object\n",
    "                        an Adam object of the optimizer class\n",
    "\n",
    "    n_epochs - integer\n",
    "                the number of epochs to train\n",
    "    \"\"\" \n",
    "    \n",
    "    for epoch in range(n_epochs): # iterate over the epochs\n",
    "        epoch_loss = 0\n",
    "        ff_model.train() # put the model in training mode\n",
    "        \n",
    "        for iteration, batch in enumerate(pairs_batch_train): # at each step take a batch of sentences\n",
    "            sent_loss = 0\n",
    "            ffnn_optimizer.zero_grad() # clear gradients\n",
    "            train_input, train_input_lengths, train_labels = batch # extract the data from the batch\n",
    "            \n",
    "            for i in range(train_input.size(0)): # iterate over each word in the sentence\n",
    "                output = ff_model(train_input[i]) # forward pass\n",
    "                \n",
    "                labels = torch.LongTensor(train_labels.size(1)) # define a random tensor with batch_size as number of elements\n",
    "                labels[:] = train_labels[i][:] # put the correct label values in the tensor\n",
    "                \n",
    "                sent_loss += loss_function(output, labels) # compute the loss, compare the predictions and the labels\n",
    "            \n",
    "            sent_loss.backward() # compute the backward pass\n",
    "            ffnn_optimizer.step() # update the parameters\n",
    "\n",
    "            epoch_loss += sent_loss.item()\n",
    "\n",
    "        print('Epoch: {}   Loss: {}'.format(epoch+1, epoch_loss / len(pairs_batch_train))) # print the loss at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48fc76e218490243e5e1d51939a811ee",
     "grade": false,
     "grade_id": "cell-b05374fdbc7addc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since training a neural network model takes a long time and a lot of resources, we are not going to train the model. Instead, we are going to load an already trained model.\n",
    "\n",
    "The cell below loads the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b489c46889f6d1766c13a272e681e146",
     "grade": false,
     "grade_id": "cell-3c2b1b88769cd717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ff_model = torch.load('../coursedata/nn-lm/ff_model.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cad2bb57cabd6cf06e7352c754146a7b",
     "grade": false,
     "grade_id": "cell-f94402aebe2eb200",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Generate text (3 Points) <a class=\"anchor\" id=\"task_2_2\"></a>\n",
    "Now that the model is trained, we can use it to generate sentences.\n",
    "\n",
    "Your task is to implement the `predict_ffnn` function.\n",
    "\n",
    "You need to perform the following steps:\n",
    "\n",
    "    1. Run the forward pass to get the output\n",
    "    2. Run the output through a softmax to convert it to a probability distribution (`F.softmax`) [remember to specify the correct dimention to the softmax function]\n",
    "    3. Flatten the output (`output.flatten()`)\n",
    "    4. Sample from a multinomial distribution with `output.multinomial(1)`\n",
    "    6. Convert the index of the predicted word to the actual word using the idx2word dictionary\n",
    "    7. Append the predicted word to the `predictions` array\n",
    "    \n",
    "<b>Remember to use the prediction from `output.multinomial(1)` as the next input to the `ff_model` function.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b50e5dca5b4f80e06efffff88b2a3d",
     "grade": false,
     "grade_id": "cell-a7d1a7dd364e812f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_ffnn(ff_model, word2idx, idx2word, start_word, max_len):\n",
    "    \"\"\"\n",
    "    This function predicts the next word, based on the previous word.\n",
    "    We start with the 'start_word' and then feed the prediction as the next input.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    ff_model - object\n",
    "                a FFNN object that contains the trained model\n",
    "                \n",
    "    word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "                    \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "                    \n",
    "    start_word - string\n",
    "                    the starting word\n",
    "    \n",
    "    max_len - integer\n",
    "                integer value representing up to how many words to generate\n",
    "                            \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    predictions - string\n",
    "                    a string containing the generated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    start_word_indexed = torch.LongTensor(1)\n",
    "    start_word_indexed[:] = word2idx[start_word] # replace the starting word with its index\n",
    "    \n",
    "    with torch.no_grad(): # don't need to compute the gradients\n",
    "        ff_model.eval() # put the model in evaluation mode\n",
    "        predictions = [] # list where we are going to store the predictions\n",
    "        predictions.append(idx2word[start_word_indexed.item()]) # add the starting word to the array\n",
    "        topk = start_word_indexed # use the starting word as the first previous word during the prediction\n",
    "\n",
    "        while((len(predictions) < max_len) and (predictions[-1] != '</s>')): # generate until we have enough words or generated </s>\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # raise NotImplementedError()\n",
    "            \n",
    "            output = ff_model(topk) \n",
    "            output = F.softmax(output, dim=1) \n",
    "            output = output.flatten() \n",
    "            topk = output.multinomial(1) \n",
    "            predicted_word = idx2word[topk.item()] \n",
    "            predictions.append(predicted_word) \n",
    "            topk = torch.LongTensor([word2idx[predicted_word]]) \n",
    "            \n",
    "    predictions = ' '.join(predictions) # convert the array of predictions to a string\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "532742c84bd2bab5930a43c719ca442b",
     "grade": true,
     "grade_id": "cell-5ff14392826ab7de",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_word = '<s>' # starting word\n",
    "dummy_max_len = 30 # maximum number of words to genertate\n",
    "\n",
    "dummy_predictions = predict_ffnn(ff_model, word2idx, idx2word, dummy_word, dummy_max_len)\n",
    "\n",
    "# check that a string is returned\n",
    "assert_equal(type(dummy_predictions), str)\n",
    "\n",
    "# check that the prediction starts with <s>\n",
    "assert_equal(dummy_predictions.split()[0], '<s>')\n",
    "\n",
    "# check that the model has generated enough samples or reached </s>\n",
    "if len(dummy_predictions.split()) < dummy_max_len:\n",
    "    assert_equal(dummy_predictions.split()[-1], '</s>')\n",
    "else:\n",
    "    assert_equal(len(dummy_predictions.split()), dummy_max_len)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b978e6eaadec470182445c0ee071b058",
     "grade": false,
     "grade_id": "cell-838815333c879095",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we are going to run the prediction function and see what the model generates. You can execute the cell below multiple times in order to get more predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> overlooked assisting stay procuring solidity feels harringtons sinking closure improbable amidst procuring solidity talker procuring conjecture oppressively regain tempers procuring lad procuring gloom named check improbable amidst procuring killed\n",
      "<s> frisks anywhere longbourn procuring instantly glass improbable amidst procuring sedate patient stubbornness effect brightened feels strong commonly blame cheering effect minds improbable amidst procuring sedate declaration profligate disappointments superseded\n",
      "<s> frisks wit tempted although none prudent acquit killed instantly sportsmen sustained similarity watchfulness deserted venting trifling honest late undecided swelling late wit unjust ignorance precipitate suspicious solidity profligate procuring\n"
     ]
    }
   ],
   "source": [
    "start_word = ['<s>', '<s>', '<s>'] # starting word\n",
    "max_len = 30 # maximum number of words to genertate\n",
    "\n",
    "for word in start_word:\n",
    "    predictions = predict_ffnn(ff_model, word2idx, idx2word, word, max_len)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "babeb168f13b898af793acf431d929a1",
     "grade": false,
     "grade_id": "cell-5ed93286f5555a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3:  Recurrent neural network language model <a class=\"anchor\" id=\"task_3\"></a>\n",
    "\n",
    "In this task, we are going to implement a recurrent neural network (RNN) language model. The recurrent neural network processes the input sequentially, where each prediction is conditionally dependent of the previous predictions. This makes them suitable for language modeling tasks.\n",
    "\n",
    "To predict the next word, we first need to convert the previous word into a vecor $x(t)$. In other words, we need to embed it. This step is identical to the one in the FFNN model. <br>\n",
    "After we have the word embedding, we need to pass it through the RNN, which is our case is a [Gated Recurrent Unit](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) (GRU). The GRU returns two tensors: `output` and `hidden`. The `output` tensor contains the output features from the last layer of the GRU, for each timestep. The `hidden` tensor contains the hidden state of the last timestep of each layer. <br>\n",
    "To get an output of size equal to the number of unique words in our vocabulary, we need to pass the output of the GRU through a linear projection, similar to the second linear projection in the FFNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "034913adca395577bd95b179e48c17bd",
     "grade": false,
     "grade_id": "cell-699a30f862ba1ad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Create the model (3 Points) <a class=\"anchor\" id=\"task_3_1\"></a>\n",
    "The `RNN` class contains the definition of the RNN model that we are going to train for the language modeling task. The first function that the class has is called `__init__` and it initializes the layers of the model. This function is already implemented for you.\n",
    "\n",
    "The next function is the `forward` function and does similar job like the one in the FFNN model. You need to implement this function.\n",
    "\n",
    "To implement the function, you need to perform the following steps:\n",
    "\n",
    "    1. Replace the indexed word with its embedding vector. In other words, pass it through the embedding layer\n",
    "    2. Reshape the embedding vector to a shape of (1, batch_size, embed_dim)\n",
    "    3. Pass the embedding through the GRU cell to get the output and the hidden tensors. The GRU function takes as input the word embedding and the previous hidden state.\n",
    "    4. Addpy a dropout to the output of the GRU.\n",
    "    5. Apply the linear transformation to the output of the dropout layer (pass it though the `self.out` layer).\n",
    "    6. Reshape the output to have a shape (batch_size, vocab_length+1)\n",
    "    7. Return the output of the linear transformation and the hidden tensor\n",
    "    \n",
    "Remember that now we are processing the words in batches. If we have a batch size of 5, that means that we are passing 5 words at a time and applying all the transformations to those 5 words simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e940b4d6bb3611a8bbd0537d71616cc5",
     "grade": false,
     "grade_id": "cell-34f34d4f50e475b3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, word2idx, embed_dim, context_dim, num_layers):\n",
    "        \"\"\"\n",
    "        This function initializes the layers of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "        \n",
    "        embed_dim - integer\n",
    "                        the size of the word embeddings\n",
    "\n",
    "        context_dim - integer\n",
    "                        the dimension of the hidden size\n",
    "                        \n",
    "        num_layers - integer\n",
    "                        the number of layers in the GRU cell\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.word2idx = word2idx\n",
    "        self.embed_dim = embed_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # here we initialise weighs of a model\n",
    "        self.word_embed = nn.Embedding(len(self.word2idx)+1, self.embed_dim) # embedding layer\n",
    "\n",
    "        self.gru = nn.GRU(self.embed_dim, self.context_dim, num_layers=self.num_layers) # GRU cell\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1) # Dropout\n",
    "        \n",
    "        self.out = nn.Linear(self.context_dim, len(self.word2idx)+1) # output layer\n",
    "\n",
    "    \n",
    "    def forward(self, word, hidden):\n",
    "        \"\"\"\n",
    "        This function implements the forward pass of the model\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        word - tensor\n",
    "                a tensor containing indices of the words in a batch\n",
    "                \n",
    "        hidden - tensor\n",
    "                    the previous hidden state of the GRU model\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output - tensor\n",
    "                    a tensor of logits from the linear transformation\n",
    "        \n",
    "        hidden - tensor\n",
    "                    the current hidden state of the GRU model\n",
    "        \"\"\" \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "    \n",
    "        embeds = self.word_embed(word).view(1, -1, self.embed_dim) \n",
    "        gru_output, hidden = self.gru(embeds, hidden)\n",
    "        output = self.dropout(gru_output)\n",
    "        output = self.out(output)\n",
    "        output = output.view(-1, len(self.word2idx) + 1) # Reshape for compatibility\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d83bcb3425bf6ac941f3ccaad474a5ed",
     "grade": true,
     "grade_id": "cell-9ed016e28f21b759",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_embed_dim = 10\n",
    "dummy_hidden_size = 20\n",
    "dummy_num_layers = 1\n",
    "\n",
    "dummy_rnn_model = RNN(word2idx, dummy_embed_dim, dummy_hidden_size, dummy_num_layers)\n",
    "dummy_train_input = torch.randint(1, 10, (2,))\n",
    "\n",
    "dummy_hidden = torch.zeros((dummy_num_layers, dummy_train_input.size(0), dummy_hidden_size))\n",
    "dummy_output, hidden = dummy_rnn_model(dummy_train_input, dummy_hidden)\n",
    "\n",
    "# test that the returned result is a tensor\n",
    "assert_equal(torch.is_tensor(dummy_output), True)\n",
    "\n",
    "# test that the shapes match\n",
    "assert_equal(dummy_output.size(), (2, len(word2idx)+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5a01554343e97ec3081db93aa1ed9e9",
     "grade": false,
     "grade_id": "cell-6573a7a0788a6b07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Model initialization\n",
    "In the next cell, we are going to define the hyperparameters of our neural network language model. Additionally, we will initialize the model, along with the loss function and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8d2068f16db18bacc4ee28e98698584",
     "grade": false,
     "grade_id": "cell-c113b9200e8b4d2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs = 10 # the number of epochs to train\n",
    "embed_dim = 300 # the size of the embedding\n",
    "hidden_size = 450 # the size of the hidden state\n",
    "num_layers = 1 # the number of layers in the GRU cell\n",
    "rnn_model = RNN(word2idx, embed_dim, hidden_size, num_layers) # initialize the RNN model\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0) # define the loss function\n",
    "rnn_optimizer = optim.Adam(rnn_model.parameters(), lr=0.001) # define the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87b159f4ab408d91fdb06b3bb71be977",
     "grade": false,
     "grade_id": "cell-2c9f6b7cb0785413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training the model\n",
    "The training process is similar to the one in the FFNN model. We pass each word through the forward pass and obtain the `output` and the `hidden` states. Then, we use the `output` to compare it against the true labels and see how far we are from the correct result. After that we compute the partial derivatives and update the parameters. This process gets repeated for $ n $ number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77f244b88d205dca3b48a80582ec0e99",
     "grade": false,
     "grade_id": "cell-8a432c7dfbc91003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_rnn(pairs_batch_train, rnn_model, hidden_size, num_layers, loss_function, rnn_optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    This function implements the training of the model\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    pairs_batch_train - object\n",
    "                            a DataLoader object that contains the batched data\n",
    "\n",
    "    rnn_model - object\n",
    "                an RNN object that contains the initialized model\n",
    "                \n",
    "    hidden_size - integer\n",
    "                    the size of the hidden layer (the context size)\n",
    "    \n",
    "    num_layers - integer\n",
    "                        the number of layers in the GRU cell\n",
    "\n",
    "    loss_function - object\n",
    "                        the CrossEntropy loss function\n",
    "\n",
    "    rnn_optimizer - object\n",
    "                        an Adam object of the optimizer class\n",
    "\n",
    "    n_epochs - integer\n",
    "                the number of epochs to train\n",
    "    \"\"\" \n",
    "\n",
    "    for epoch in range(n_epochs): # iterate over the epochs\n",
    "        epoch_loss = 0\n",
    "        rnn_model.train() # put the model in training mode\n",
    "        \n",
    "        for iteration, batch in enumerate(pairs_batch_train): # at each step take a batch of sentences\n",
    "            sent_loss = 0\n",
    "            rnn_optimizer.zero_grad() # clear gradients\n",
    "            \n",
    "            train_input, train_input_lengths, train_labels = batch # extract the data from the batch\n",
    "            hidden = torch.zeros((num_layers, train_input.size(1), hidden_size)) # initialize the hidden state\n",
    "            \n",
    "            for i in range(train_input.size(0)): # iterate over the word in the sentence\n",
    "                output, hidden = rnn_model(train_input[i], hidden) # forward pass\n",
    "                labels = torch.LongTensor(train_labels.size(1)) # define a random tensor with batch_size as number of elements\n",
    "                labels[:] = train_labels[i][:] # put the correct label values in the tensor\n",
    "                \n",
    "                sent_loss += loss_function(output, labels) # compute the loss, compare the predictions and the labels\n",
    "\n",
    "            sent_loss.backward() # compute the backward pass\n",
    "            rnn_optimizer.step() # update the parameters\n",
    "\n",
    "            epoch_loss += sent_loss.item()\n",
    "            \n",
    "        print('Epoch: {}   Loss: {}'.format(epoch+1, epoch_loss / len(pairs_batch_train))) # print the loss at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e083cdb809bfe17ad433fd699f1c52b",
     "grade": false,
     "grade_id": "cell-a8ca8840d7662157",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Similiar to the FFNN, we are not going to train the model. Instead, we are going to load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba779074665d716df577890986d3e8fc",
     "grade": false,
     "grade_id": "cell-65dbc9a5722d2090",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rnn_model = torch.load('../coursedata/nn-lm/rnn_model.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c73087415aa908a43835b0e97a136ca",
     "grade": false,
     "grade_id": "cell-76ca48bf8c2ae557",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Generate text (3 Points) <a class=\"anchor\" id=\"task_3_2\"></a>\n",
    "Now that the model is trained, we can use it to generate sentences.\n",
    "\n",
    "Your task is to implement the `predict_rnn` function.\n",
    "\n",
    "You need to perform the following steps:\n",
    "\n",
    "    1. Run the forward pass to get the output. Don't forget to pass the `hidden` state\n",
    "    2. Run the output through a softmax to convert it to a probability distribution (`F.softmax`) [don't forget to specify the dimension in the softmax function]\n",
    "    3. Get the word with the highest probability using the `topk()` function\n",
    "    4. Set the `next_input` to be the predicted word with the highest probability (the topk word) [very important!].\n",
    "    5. Convert the index of the predicted word to the actual word using the idx2word dictionary\n",
    "    6. Append the predicted word to the `predictions` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c704cbf91f9fcbe4cf56be0bc0e5d59",
     "grade": false,
     "grade_id": "cell-14078785f15d0f87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len):\n",
    "    \"\"\"\n",
    "    This function predicts the next word, based on the history of the previous words.\n",
    "    We start with the 'context' and then feed the prediction as the next input.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rnn_model - object\n",
    "                an RNN object that contains the trained model\n",
    "                \n",
    "    hidden_size - integer\n",
    "                    the size of the hidden layer (the context size)\n",
    "                    \n",
    "    num_layers - integer\n",
    "                    the number of layers in the GRU cell\n",
    "                \n",
    "    word2idx - dictionary\n",
    "                    a dictionary where the keys are the unique words in the data\n",
    "                    and the values are the unique indices corresponding to the words\n",
    "                    \n",
    "    idx2word - dictionary\n",
    "                a dictionary, where the keys are the indices and the values are the words\n",
    "                    \n",
    "    context - string\n",
    "                the context sentence\n",
    "    \n",
    "    max_len - integer\n",
    "                integer value representing up to how many words to generate\n",
    "                            \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    predictions - string\n",
    "                    a string containing the generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # index the context\n",
    "    context_indexed = []\n",
    "    for word in context.split():\n",
    "        word_indexed = torch.LongTensor(1)\n",
    "        word_indexed[:] = word2idx[word]\n",
    "        context_indexed.append(word_indexed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        # first build the hidden state from the context\n",
    "        hidden = torch.zeros((num_layers, 1, hidden_size))\n",
    "        for word in context_indexed:\n",
    "            predictions.append(idx2word[word.item()])\n",
    "            output, hidden = rnn_model(word, hidden)\n",
    "            \n",
    "        next_input = context_indexed[-1]\n",
    "        while((len(predictions) < max_len) and (predictions[-1] != '</s>')):\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # raise NotImplementedError()\n",
    "            \n",
    "            output, hidden = rnn_model(next_input, hidden) \n",
    "            output_softmax = F.softmax(output, dim=1) \n",
    "            top_value, top_index = output_softmax.topk(1)  \n",
    "            next_input = top_index.squeeze().detach()\n",
    "            \n",
    "            if next_input.item() in idx2word:\n",
    "                predicted_word = idx2word[next_input.item()]\n",
    "                predictions.append(predicted_word)\n",
    "            else:\n",
    "                break\n",
    "            if predicted_word == '</s>': \n",
    "                break\n",
    "                \n",
    "    predictions = ' '.join(predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c5d59f6674ca21fb544c16a8cc0f35c",
     "grade": true,
     "grade_id": "cell-6ce9bd8c9f198a0b",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_context = '<s> the' # starting word\n",
    "dummy_max_len = 15 # maximum number of word to genertate\n",
    "\n",
    "dummy_predictions = predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, dummy_context, dummy_max_len)\n",
    "\n",
    "# check that a string is returned\n",
    "assert_equal(type(dummy_predictions), str)\n",
    "\n",
    "# check that the prediction starts with <s>\n",
    "assert_equal(dummy_predictions.split()[0], '<s>')\n",
    "\n",
    "# check that the model has generated enough samples or reached </s>\n",
    "if len(dummy_predictions.split()) < dummy_max_len:\n",
    "    assert_equal(dummy_predictions.split()[-1], '</s>')\n",
    "else:\n",
    "    assert_equal(len(dummy_predictions.split()), dummy_max_len)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7926b4a3323f115e525d4f397fcbbe47",
     "grade": false,
     "grade_id": "cell-d7b6eda5b6ae9133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> this has been entirely owned replying dupe dupe dupe dupe substitute arch bridegroom dupe lucases mouths music honestly music honestly music honestly death seizing testimony seizing seizing prudently uglier dissolved waste weary humanity chaise honestly honestly music afforded uglier turned licence jestingly fourthly named dupe ankles announce entreaties sisters\n",
      "\n",
      "\n",
      "<s> the person mentioned signs condemned formation waste comprehends restraint kympton gentlest hesitate abusing waste begun warmest continued condemned despicably descending loose treasured mud quartered week prodigious quartered twenty violence censured practises insipidity violence conclusion looking powers behave remaining deaden unqualified unbecoming music quartered descended licence dance descending doleful blinded\n",
      "\n",
      "\n",
      "<s> it is interesting encumbrance flirtation flirtation antagonist honour selfish ashamed dupe substitute behave title struggled warmest superciliousness blinded thoughtlessness ensued abusing arch abrupt anxiously painful instituted quartered rooms humanity unqualified week survey humanity unqualified humanity unqualified piling comparatively distracted expressions quartered prevail mud teased arch economically prevail weary quartered\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contexts = ['<s> this has been', '<s> the person', '<s> it is interesting']\n",
    "max_len = 50\n",
    "\n",
    "for context in contexts:\n",
    "    predictions = predict_rnn(rnn_model, hidden_size, num_layers, word2idx, idx2word, context, max_len)\n",
    "    print(predictions)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75b20dd8c3017d177970305c8a34a8fd",
     "grade": false,
     "grade_id": "cell-5441f80a5fb10712",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 4: Output analysis <a class=\"anchor\" id=\"task_4\"></a>\n",
    "This task will be manually graded. It is focused on understanding the difference between the models and why one might perform better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeb3b98b619028cda011283827668429",
     "grade": false,
     "grade_id": "cell-bddaf8651d8c83ea",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## 4.1 Model comparison (3 Points) <a class=\"anchor\" id=\"task_4_1\"></a>\n",
    "Answer the following questions:\n",
    "\n",
    "    1. Which model generates more sensible text?\n",
    "    2. Why is that?\n",
    "    3. Write at least one shortcoming of both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to FFNN, RNN generates far more logical sentences based on the generated output.\n",
    "\n",
    "1. Text produced by the RNN model is more logical. When it comes to sequential data tasks, like text prediction in our example, RNN outperforms FFNN. This is because RNN keeps an internal memory of previous inputs and outputs. This enables RNN to model dependencies between elements of a data sequence by allowing the network to create a feedback loop (by using the previous time step's output as the current time step's input). On contrary, the FFNN design is less effective for processing sequential data because it can only handle fixed-size inputs.\n",
    "\n",
    "2. Unlike the FFNN model, which simply predicts the future word based on the present word, the RNN model continuously stores and updates inputs, giving it some recollection of past information. As a result, the RNN model can produce sentences that is more logical and coherent. Both FFNN and RNN are prone to overfitting, particularly when the models have many parameters in comparison to the volume of training data.\n",
    "\n",
    "3. Because both models have poor memory capacities, they are not very well suited to produce both long and logical statements. The FFNN architecture exhibits particularly poor performance in predicting long phrases because to its reliance on fixed-size input and lack of memory capacity to manage sequential data. Furthermore, the challenge of vanishing/exploding gradients with time make it even more difficult for RNNs to forecast longer texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e689da5fb9e59bc28ec6cb7c4ec15c8",
     "grade": false,
     "grade_id": "cell-62e5eafcd2989fb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything.\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please provide a feedback so that we can improve the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
