{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8107ef835d29b6af241d9ba7e1cf777a",
     "grade": false,
     "grade_id": "cell-a74ccc5dcde03d2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 6: Subword segmentation\n",
    "\n",
    "## Released: 27.2.2024 at 14:30\n",
    "## Deadline: 8.3.2024 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e27e6bd313f00c4c76f8f72db7d074ff",
     "grade": false,
     "grade_id": "cell-bd0631ed211d7cca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "We've already talked about the problem of segmenting text into appropriate units (tokenization). Back then, it was words that were considered as those units, but there are other units that you should probably explore as well: characters and **subwords**. In this assignment, we're going to focus on **tokenization into subwords**.\n",
    "\n",
    "The motivation to segmenting words further into smaller elements comes from morphology, where such elements are called **morphemes**. A **morpheme** is defined as the smallest meaning-bearing unit of a language. For example, the word *unpredictable* contains three morphemes: *un*, *predict* and *able*. As you can see, many morphemes are not unique to one word, they are elements that are regularly seen in other words too. For example, *un* in *unhappy*, *predict* in *predictive*, and *able* in *comfortable*. Thus, just as sentences are constructed from words, words are constructed from morphemes. \n",
    "\n",
    "Segmenting into morphemes (especially in languages with rich morphology) helps to avoid the problem of out-of-vocabulary (OOV) words in text corpora. For example, if our training corpus contains *cool*, *coolest* and *dumber*, when the new word *cooler* comes in, it can be segmented into *cool-er* and understood based on its constituent subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e92be7689663906ea2286ac72f616554",
     "grade": false,
     "grade_id": "cell-5f9b8e119097a2a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Task 1: BPE](#task_1)\n",
    "    * [Step 1.1](#subtask_1_1)\n",
    "        * [Step 1.1.1: Collecting word counts](#subtask_1_1_1)\n",
    "        * [Step 1.1.2: Convering words to characters](#subtask_1_1_2)\n",
    "    * [Step 1.2: Collecting symbol pairs frequencies](#subtask_1_2)\n",
    "    * [Step 1.3: Merging the most frequent pair](#subtask_1_3)\n",
    "    * [Step 1.4: Combining steps 1-3 together](#subtask_1_4)\n",
    "    * [Step 1.5-7: Segmenting a corpus](#subtask_1_5)\n",
    "* [Task 2: ANALYZE SEGMENTATIONS](#task_2)\n",
    "    * [Step 2.1: Count word OOV](#subtask_2_1)\n",
    "    * [Step 2.2: Count subword OOV](#subtask_2_2)\n",
    "    * [Step 2.3: Does the segmentation make sense?](#subtask_2_3)\n",
    "    * [Step 2.4: Your thoughts](#subtask_2_4)\n",
    "* [Checklist](#checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53e8c70ef1f6052446549f6d60bb70e7",
     "grade": false,
     "grade_id": "cell-2cbd322d57c4b67a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1 <a class=\"anchor\" id=\"task_1\"></a>\n",
    "## BPE\n",
    "One approach to tokenization into subwords is based on the **byte pair encoding** (**BPE**) algorithm for data compression. Starting from single characters as the subwords, it iteratively merges frequent pairs of subwords forming new subwords. Assuming morphemes are frequently repeated substrings, this method should often merge symbols into morphemes instead of into other, meaningless character sequences. The algorithm is applied only inside words (there are no merges across word boundaries). \n",
    "\n",
    "**BPE** algorithm begins with its vocabulary being a set of characters seen in the training corpus. Each word in the corpus is represented as a sequence of characters plus a special end-of-word symbol '_'. At each iteration step $k_i$, the algorithm counts the number of symbol pairs, finds the most frequent pair ['A','B'] and replaces it with the new merged symbol ‘AB’. The algorithm stops when it's done $k$ merges ($k$ is a parameter of the algorithm). The algorithm begins with the set of symbols equal to the set of characters. The resulting symbol set should have the original set of characters plus $k$ new symbols. \n",
    "\n",
    "To obtain a subword vocabulary with **BPE**, you should take the following steps:\n",
    "* STEP 1: tokenize a training corpus into words and collect frequency statistics of word tokens in the training corpus. Additionally, represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 2: count the frequencies of symbol pairs.\n",
    "* STEP 3: replace every occurrence of the most frequent pair ['A', 'B'] with the new merged symbol 'AB'.\n",
    "* STEP 4: repeat STEPs 2-3 $k-1$ times more.\n",
    "\n",
    "To segment a corpus using the vocabulary, you should:\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word apply each merge operation in the order they were learned, and return the tokenized text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a4560f44b39d4596fb668023dbf7a5e",
     "grade": false,
     "grade_id": "cell-fc78b008631ec01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 <a class=\"anchor\" id=\"subtask_1_1\"></a>\n",
    "### STEP 1\n",
    "### 1.1.1  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Collecting word counts (1 Point)\n",
    "\n",
    "Write a function that reads a text as one string from a file, tokenizes it into words by whitespaces and collects the word frequencies. It should return a dictionary of words and their raw counts in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1effda1a6f0f3cd62ff99a153f091f3",
     "grade": false,
     "grade_id": "cell-1b45cebfbb7868fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_word_counts(file_name):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file, reads the file, splits it into words by whitespaces, \n",
    "    and then counts the words' frequencies.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "            a path to a training corpus as a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word_counts : dictionary\n",
    "            a dictionary of word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    word_counts = {}\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc11c24648325ed3a83e7f8976d526e",
     "grade": true,
     "grade_id": "cell-ac8d751900ffa327",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"../coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(collect_word_counts(dummy_corpus_path)), dict)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['low'], 5)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['lowest'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['new'], 2)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['newer'], 6)\n",
    "assert_equal(collect_word_counts(dummy_corpus_path)['WIDer'], 3)\n",
    "\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"../coursedata/06-subwords/gum_train.txt\"\n",
    "# check that the vocabulary length is right\n",
    "assert_equal(len(collect_word_counts(gum_train_path)), 8677)\n",
    "# check that the word count is right\n",
    "assert_equal(collect_word_counts(gum_train_path)['we'], 112)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4db9a8174363efdf8678ecb380534f8",
     "grade": false,
     "grade_id": "cell-41847e0b7837e171",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1.2  <a class=\"anchor\" id=\"subtask_1_1_1\"></a>\n",
    "### Converting words to characters (1 Point)\n",
    "Now, represent each word in your frequency dictionary as a tuple of characters plus a special end-of-word marker '_'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a40d6b8a176061a4b790ccdd5d1c7bd",
     "grade": false,
     "grade_id": "cell-e358614034bbeaad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_chars(vocab):\n",
    "    \"\"\"\n",
    "    Takes in a frequeny dictionary of words in the training corpus\n",
    "    and converts the key words to a tuple of characters plus a special end-of-word symbol '_'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : dictionary\n",
    "        a frequency dictionary of words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    separated_vocab : dictionary \n",
    "        a frequency dictionary of words represented as a tuple of characters \n",
    "        plus a special end-of-word symbol '_'\n",
    "        {('l','o','w','_') : 3}\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    separated_vocab = {}\n",
    "    for word, count in vocab.items():\n",
    "        char_tuple = tuple(word) + ('_',)\n",
    "        separated_vocab[char_tuple] = count\n",
    "    return separated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a539a8a68bee21b1ed5d78736d65466",
     "grade": true,
     "grade_id": "cell-8efbbf91eee65873",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab = {'low': 5, 'lowest': 2, 'newer': 6, 'wider': 3, 'New': 2}\n",
    "\n",
    "# check that the output of the function is a dict\n",
    "assert_equal(type(convert_to_chars(dummy_freq_vocab)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(convert_to_chars(dummy_freq_vocab).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(convert_to_chars(dummy_freq_vocab)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', '_')], 5)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('l', 'o', 'w', 'e', 's', 't', '_')], 2)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('n', 'e', 'w', 'e', 'r', '_')], 6)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('w', 'i', 'd', 'e', 'r', '_')], 3)\n",
    "assert_equal(convert_to_chars(dummy_freq_vocab)[('N', 'e', 'w', '_')], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fce5d811c1a1df215f85df879b42cbae",
     "grade": false,
     "grade_id": "cell-48322d22d20d67b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 <a class=\"anchor\" id=\"subtask_1_2\"></a>\n",
    "### STEP 2\n",
    "### Collecting symbol pairs frequencies (1 Point)\n",
    "Write a function that takes in a frequency dictionary where keys are words represented as tuples of symbols, and outputs the most frequent pair of symbols in the corpus. In the case, when there are several pairs with the same frequency, return the pair that is earlier alphabetically. \n",
    "\n",
    "For example, if we only have one *l o o k _* and one *l o o p _*  in our frequency dictionary, the fuction should output *l o* as the most frequent pair (it is earlier than *o o* alphabetically). Note: the alphabetical sorting is done *before* the merging instead of for the merged subwords: the pair ('aa', 'b') comes alphabetically after ('a', 'zz') even though the merged subword 'aab' would come before 'azz'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f469bc5c4dbd606fcd5ae0034b4496ad",
     "grade": false,
     "grade_id": "cell-e2a9fb6eb0db7f82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_the_pair_to_merge(vocab_as_symbols):\n",
    "    \"\"\"\n",
    "    Takes in a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts, \n",
    "    and outputs the most frequent pair of symbols in the corpus.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merge_pair : tuple of strings \n",
    "        the most frequent pair of symbols, a pair to merge\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    pair_freqs = {}\n",
    "    for word, freq in vocab_as_symbols.items():\n",
    "        for i in range(len(word) - 1): \n",
    "            pair = (word[i], word[i + 1])\n",
    "            if pair in pair_freqs:\n",
    "                pair_freqs[pair] += freq\n",
    "            else:\n",
    "                pair_freqs[pair] = freq\n",
    "    \n",
    "    merge_pair = sorted(pair_freqs.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
    "    \n",
    "    return merge_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "461a0855fdf521d9ab853b24e5392ab9",
     "grade": true,
     "grade_id": "cell-74bc7d2d9ec1da10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_freq_vocab_as_symbols = {('l','o','o','k','_') : 1, \n",
    "                               ('l','o','o','p','_') : 1}\n",
    "\n",
    "# check that the output of the function is a tuple\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)), tuple)\n",
    "# check that the output of the function is a tuple of strings\n",
    "assert_equal(type(get_the_pair_to_merge(dummy_freq_vocab_as_symbols)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols), ('l', 'o'))\n",
    "\n",
    "\n",
    "dummy_freq_vocab_as_symbols2 = {('l', 'o', 'w', '_'): 5,\n",
    "                                ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                                ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                                ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                                ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols2), ('e', 'r'))\n",
    "\n",
    "# check that your functions can merge not only characters\n",
    "\n",
    "dummy_freq_vocab_as_symbols3 = {('lo', 'o', 'k', '_'): 1 , \n",
    "                                ('lo', 'o', 'p','_'): 1}\n",
    "\n",
    "assert_equal(get_the_pair_to_merge(dummy_freq_vocab_as_symbols3), ('lo', 'o'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9faa70b5827043a943b87f6a7dcb76bd",
     "grade": false,
     "grade_id": "cell-eb3e7bd4a0d2d372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 <a class=\"anchor\" id=\"subtask_1_3\"></a>\n",
    "### STEP 3\n",
    "### Merging the most frequent pair (3 Points)\n",
    "\n",
    "Write a function that takes in a pair of symbols to merge and a frequency dictionary where words are represented as tuples of symbols. It should return a frequency dictionary, where words are still represented as tuples of symbols but the most frequnt pair is now merged in every word.\n",
    "\n",
    "For example, if we want to merge *l* and *o* in our *l o o k _* and *l o o p _*  frequency dictionary, the fuction should output *lo o k _* and *lo o p _*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580f10b6fe27d9d676fcc2abe4716412",
     "grade": false,
     "grade_id": "cell-7f9d1e7549722c1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge(vocab_as_symbols, merge_pair):\n",
    "    \"\"\"Merges the most frequent pair of symbols in all words\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    vocab_as_symbols : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols and values are their counts\n",
    "    merge_pair : tuple\n",
    "        a pair of symbols to merge\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    new_vocab : dictionary\n",
    "        a frequency dictionary, where keys are words represented as tuples of symbols, \n",
    "        with the given pair represented as a new symbol (concatenated pair)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    new_vocab = {}\n",
    "    for word, freq in vocab_as_symbols.items():\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == merge_pair[0] and word[i + 1] == merge_pair[1]:\n",
    "                new_word.append(merge_pair[0] + merge_pair[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_word)] = freq\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1914d4b9b996cdac52b8aabc966bdb0",
     "grade": true,
     "grade_id": "cell-e71ea4c2a2aee8c2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_as_symbols = {('l', 'o', 'w', '_'): 5,\n",
    "                          ('l', 'o', 'w', 'e', 's', 't', '_'): 2,\n",
    "                          ('n', 'e', 'w', 'e', 'r', '_'): 6, \n",
    "                          ('w', 'i', 'd', 'e', 'r', '_'): 3, \n",
    "                          ('n', 'e', 'w', '_'): 2}\n",
    "\n",
    "dummy_pair = ('e', 'r')\n",
    "\n",
    "# check that the output of the function is a dictionary\n",
    "assert_equal(type(merge(dummy_vocab_as_symbols, dummy_pair)), dict)\n",
    "# check that the keys are tuples\n",
    "assert_equal(type(list(merge(dummy_vocab_as_symbols, dummy_pair).keys())[0]), tuple)\n",
    "#check that there are no new elements\n",
    "assert_equal(len(merge(dummy_vocab_as_symbols, dummy_pair)), 5)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the pair was merged everywhere\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols, dummy_pair).keys()), [('l', 'o', 'w', '_'), \n",
    "                                                                    ('l', 'o', 'w', 'e', 's', 't', '_'), \n",
    "                                                                    ('n', 'e', 'w', 'er', '_'), \n",
    "                                                                    ('w', 'i', 'd', 'er', '_'), \n",
    "                                                                    ('n', 'e', 'w', '_')])\n",
    "# check that you can handle several merge pairs in a word\n",
    "dummy_vocab_as_symbols2 = {('l','o','o','o','l'): 1, \n",
    "                           ('l','o','o','o','o','l'):1}\n",
    "\n",
    "dummy_pair2 = ('o', 'o')\n",
    "\n",
    "\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols2, dummy_pair2).keys()), [('l', 'oo', 'o', 'l'), \n",
    "                                                                        ('l','oo','oo','l')])\n",
    "\n",
    "# check that you can handle merge pairs of multicharacter symols\n",
    "dummy_vocab_as_symbols3 = {('l','oo','oo','l'): 1, \n",
    "                           ('l','oo','oo','oo','o','l'):1}\n",
    "\n",
    "dummy_pair3 = ('oo', 'oo')\n",
    "\n",
    "\n",
    "assert_equal(list(merge(dummy_vocab_as_symbols3, dummy_pair3).keys()), [('l', 'oooo', 'l'), \n",
    "                                                                        ('l','oooo','oo','o','l')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d084a68e84b418e9e3a6503d02d318",
     "grade": false,
     "grade_id": "cell-ddba734f24516d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 <a class=\"anchor\" id=\"subtask_1_4\"></a>\n",
    "### STEP 4\n",
    "### Combining steps 1-3 together (1 Point)\n",
    "Now let's combine steps 1-3 into a function that learns $k$ BPE merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da79b54e93721feda78f1538f7ba4cba",
     "grade": false,
     "grade_id": "cell-ccfb0b9de81867b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learn_BPE_merges(file_name, k):\n",
    "    \"\"\" Learns k BPE subwords\n",
    "    \n",
    "    STEP 1: Collect a word count dictionary from a file,\n",
    "            represent words as a tuple of their characters plus a special end-of-word symbol '_'\n",
    "    Now k times\n",
    "        STEP 2: Choose the most frequent pair of symbols\n",
    "                add this pair as a new subword unit into a subword vocabulary\n",
    "        STEP 3: Merge the symbols in all words\n",
    "\n",
    "        \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a training corpus as a string\n",
    "    k : integer\n",
    "        a number of merges to be learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    merges : list of strings\n",
    "        a list of k subwords (symbol merges) in the order they were learned\n",
    "        one merge is a tuple of two symbols in the most frequent pair at step k\n",
    "    \"\"\"\n",
    "    \n",
    "    merges = []\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # STEP 1\n",
    "    word_counts = collect_word_counts(file_name)\n",
    "    vocab_as_symbols = convert_to_chars(word_counts)\n",
    "    \n",
    "    merges = []\n",
    "    for i in range(k):\n",
    "        # STEP 2\n",
    "        merge_pair = get_the_pair_to_merge(vocab_as_symbols)\n",
    "        merges.append(merge_pair)\n",
    "        \n",
    "        # STEP 3\n",
    "        vocab_as_symbols = merge(vocab_as_symbols, merge_pair)\n",
    "        \n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f0850b1c7e27f50581f115fcea5a3a2",
     "grade": true,
     "grade_id": "cell-5bfcbdbe63a3fdff",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_corpus_path = \"../coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)), list)\n",
    "# check that the output of the function is a list of tuples\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0]), tuple)\n",
    "# check that the output of the function is a list of tuples of strings\n",
    "assert_equal(type(learn_BPE_merges(dummy_corpus_path, 10)[0][0]), str)\n",
    "#check that there are exactly k merges\n",
    "assert_equal(len(learn_BPE_merges(dummy_corpus_path, 10)), 10)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that 8 dummy merges are correct\n",
    "assert_equal(learn_BPE_merges(dummy_corpus_path, 8), [('e', 'r'),\n",
    "                                                     ('er', '_'),\n",
    "                                                     ('e', 'w'),\n",
    "                                                     ('n', 'ew'),\n",
    "                                                     ('l', 'o'),\n",
    "                                                     ('lo', 'w'),\n",
    "                                                     ('new', 'er_'),\n",
    "                                                     ('low', '_')])\n",
    "\n",
    "# A SANITY CHECK FOR THE NOTEBOOK DATA\n",
    "gum_train_path = \"../coursedata/06-subwords/gum_train.txt\"\n",
    "assert_equal(learn_BPE_merges(gum_train_path, 5), [('e', '_'),\n",
    "                                                   ('s', '_'),\n",
    "                                                   ('t', 'h'),\n",
    "                                                   ('t', '_'),\n",
    "                                                   ('d', '_')])\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03b138b65af0db5ff4b97f7190ff0af2",
     "grade": false,
     "grade_id": "cell-b15a884d8a45d11c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5-7 <a class=\"anchor\" id=\"subtask_1_5\"></a>\n",
    "### STEPS 5-7\n",
    "### Segmenting a corpus (5 Points)\n",
    "Well, now we can apply what we've learned to tokenize any text into its subwords. Write a function that reads the test corpus and applies the merges we've learned on a training corpus. Note that you will need to adapt your previous functions a little for this. For example, we don't need to count the word frequencies since they don't play any role here anymore.\n",
    "\n",
    "Just a reminder of the steps needed to apply a subword tokenzation to a new (or an old) text:\n",
    "\n",
    "* STEP 5: tokenize a test corpus into words (with the same tokenization algorithm as in training).\n",
    "* STEP 6: represent each word as a list of characters plus a special end-of-word symbol '_'.\n",
    "* STEP 7: for every word, apply each merge operation in the order they were learned, remove the end-of-word symbol '_' and return the tokenized text.\n",
    "\n",
    "\n",
    "For the purposes of the exercise, the tokenized text should be a list of strings where strings are words with their subwords separated by whitespaces: ['I', 'lo ok', 'g oo d']. This way it will be easier for you to check how each word is tokenized. In the real application, a tokenized text will be represented just as a list of subwords.\n",
    "\n",
    "\n",
    "Note: don't forget to get rid of the special end-of-word symbol after tokenization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94859ab9c9e366bff7f6dc79a9fd877",
     "grade": false,
     "grade_id": "cell-bc393db75acac1eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def segment_text(file_name, merges):\n",
    "    \"\"\"\n",
    "    Takes in a path to a text file and lerned BPE merges,\n",
    "    reads the file and tokenizes it into subwords in accorance with the merges.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    file_name : string\n",
    "        a path to a text as a string\n",
    "    merges : list of tuples\n",
    "        a list of k merges in the order they were learned\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    segmented_text - list of strings\n",
    "        a text segmented with BPE\n",
    "        the text is a list of words\n",
    "        where each word is a string with its segments separated by whitespaces:\n",
    "        ['I', 'lo ok', 'g oo d']\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    \n",
    "    def apply_merge_operations(word, merges):\n",
    "        for merge in merges:\n",
    "            merged_symbol = ''.join(merge)\n",
    "            i = 0\n",
    "            while i < len(word) - 1:\n",
    "                if word[i] == merge[0] and word[i+1] == merge[1]:\n",
    "                    word = word[:i] + [merged_symbol] + word[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "        return word\n",
    "\n",
    "    segmented_text = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            for word in words:\n",
    "                word_with_end_symbol = list(word) + ['_']\n",
    "                merged_word = apply_merge_operations(word_with_end_symbol, merges)\n",
    "                segmented_word = ' '.join(merged_word).replace('_', '').strip()\n",
    "                segmented_text.append(segmented_word)\n",
    "    return segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60682daf4bbfa5025ae8f6c4c735e83d",
     "grade": true,
     "grade_id": "cell-6d91df8425edb415",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_train_path = \"../coursedata/06-subwords/dummy_corpus.txt\"\n",
    "\n",
    "dummy_merges = [('e', 'r'),\n",
    "                ('er', '_'),\n",
    "                ('e', 'w'),\n",
    "                ('n', 'ew'),\n",
    "                ('l', 'o'),\n",
    "                ('lo', 'w'),\n",
    "                ('ma', 'ma')]\n",
    "\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)), list)\n",
    "# check that the output of the function is a list of strings\n",
    "assert_equal(type(segment_text(dummy_train_path, dummy_merges)[0]), str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the dummy train corpus is segmented the way it should:\n",
    "\n",
    "assert_equal(segment_text(dummy_train_path, dummy_merges), ['low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low',\n",
    "                                                             'low e s t',\n",
    "                                                             'low e s t',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'new er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'W I D er',\n",
    "                                                             'new',\n",
    "                                                             'new'])\n",
    "\n",
    "\n",
    "# check that the dummy test corpus is segmented the way it should:\n",
    "dummy_test_path = \"../coursedata/06-subwords/dummy_test_corpus.txt\"\n",
    "assert_equal(segment_text(dummy_test_path, dummy_merges), ['low er', 'c o o l er'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67db36abdc81e905c9501fad72041d40",
     "grade": false,
     "grade_id": "cell-3b41d5204748e381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2 <a class=\"anchor\" id=\"task_2\"></a>\n",
    "## ANALYZE SEGMENTATIONS\n",
    "## 2.1 <a class=\"anchor\" id=\"subtask_2_1\"></a>\n",
    "### Count word OOV (1 Point)\n",
    "Now that we've done with the algorithm, let's see if it will actually help us with the OOV problem.\n",
    "Let's use the same corpus as we did in the previous POS-tagging assignment. We've randomly shuffled the sentences and split the corpus roughly in half. One half will be our training example, and another will be our test example.\n",
    "\n",
    "Analyze:\n",
    "1. The number of word **types** in the vocabulary of the training corpus.\n",
    "2. The number of word **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of word **types** in the test corpus vocabulary that are absent in the training corpus. (what part of test corpus vocabulary is OOV?)\n",
    "\n",
    "Run the cell below, to collect the tokenized corpora (we're splitting the words in the corpora by whitespaces), assign your answers to the variables in the next cell. (If you want, you can add a cell for your code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cfff879d293f8960f52f6a9f6c8e777",
     "grade": false,
     "grade_id": "cell-a3ce2430b488bbe3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../coursedata/06-subwords/gum_train.txt\", 'r') as f:\n",
    "    train = f.read()\n",
    "    train = train.split()\n",
    "\n",
    "with open(\"../coursedata/06-subwords/gum_test.txt\", 'r') as f:\n",
    "    test = f.read()\n",
    "    test = test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "860c1cc568071216b38a84b6cfed0e4a",
     "grade": false,
     "grade_id": "cell-6b5793a004cb3f7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of word types in the training corpus vocabulary: 8677\n",
      "2. Number of word types in the test corpus vocabulary: 8770\n",
      "3. Percentage of word types in the test corpus vocabulary that are absent in the training corpus: 57.41163055872291\n"
     ]
    }
   ],
   "source": [
    "# Assign your answers to the variables below:\n",
    "#     len_of_train_vocab : int\n",
    "#     len_of_test_vocab : int\n",
    "#     oov_percentage : float (between between 0 and 100)\n",
    "# As in:\n",
    "# len_of_train_vocab = <an expression that evaluates to the correct value>\n",
    "\n",
    "# Assuming the 'train' and 'test' variables are lists of words from the respective corpora\n",
    "train_vocab = set(train)\n",
    "test_vocab = set(test)\n",
    "\n",
    "len_of_train_vocab = len(train_vocab)\n",
    "len_of_test_vocab = len(test_vocab)\n",
    "\n",
    "oov_words = test_vocab - train_vocab\n",
    "oov_percentage = (len(oov_words) / len_of_test_vocab) * 100\n",
    "\n",
    "print(\"1. Number of word types in the training corpus vocabulary:\", len_of_train_vocab)\n",
    "print(\"2. Number of word types in the test corpus vocabulary:\", len_of_test_vocab)\n",
    "print(\"3. Percentage of word types in the test corpus vocabulary that are absent in the training corpus:\", oov_percentage)\n",
    "\n",
    "\n",
    "# 1. The number of word types in the vocabulary of the training corpus.\n",
    "len_of_train_vocab = 8677\n",
    "\n",
    "# 2. The number of word types in the vocabulary of the test corpus.\n",
    "len_of_test_vocab = 8770\n",
    "\n",
    "# 3. What part of test corpus vocabulary is OOV?\n",
    "oov_percentage = 57.41163055872291\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd6d2cdad3f7a507dcfa3efdad3a2ba4",
     "grade": true,
     "grade_id": "cell-dc2b01fa475e484e",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# QUESTION 1 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(len_of_train_vocab), int)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(7000 < len_of_train_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cb714186b5314b7e172db1b016989df",
     "grade": true,
     "grade_id": "cell-f522e06e0c3478c1",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(len_of_test_vocab), int)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(7000 < len_of_test_vocab < 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be83779e2313f459c4aeef140d87015c",
     "grade": true,
     "grade_id": "cell-1223f6fd86f2005c",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(oov_percentage), float)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(1 < oov_percentage < 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5b2b2c620925e6aa7df8009ee606ffa",
     "grade": false,
     "grade_id": "cell-586822c916ee5799",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 <a class=\"anchor\" id=\"subtask_2_2\"></a>\n",
    "### Count subword OOV (1 Point)\n",
    "\n",
    "Now, let's compare the OOV numbers we've got in the case where the text is tokenized by words and the case when it's tokenized by subwords. \n",
    "\n",
    "Learn 5000 BPE segmentation from the training data, then segment both corpora and compare the vocabulary numbers again. Note that it will take a couple of minutes to run BPE.\n",
    "\n",
    "Analyze:\n",
    "1. The number of subword **types** in the vocabulary of the training corpus.\n",
    "2. The number of subword **types** in the vocabulary of the test corpus.\n",
    "3. The percentage of subword **types** in the test corpus vocabulary that are absent in the training corpus. (What part of test corpus vocabulary is OOV?)\n",
    "\n",
    "\n",
    "Again, if you want you can add a new cell to do the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a69503220dc9f4e9320c6e1821ddc46a",
     "grade": false,
     "grade_id": "cell-10da1fe631f3755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "merges = learn_BPE_merges(\"../coursedata/06-subwords/gum_train.txt\", 5000)\n",
    "segmented_train = segment_text(\"../coursedata/06-subwords/gum_train.txt\", merges)\n",
    "segmented_test = segment_text(\"../coursedata/06-subwords/gum_test.txt\", merges)\n",
    "# represent texts as a list of subwords\n",
    "segmented_train_as_subwords = \" \".join(segmented_train).split(\" \")\n",
    "segmented_test_as_subwords = \" \".join(segmented_test).split(\" \")\n",
    "\n",
    "\n",
    "# double checking that your BPE algorithm is working correctly\n",
    "assert_equal(merges[0], ('e', '_'))\n",
    "assert_equal(merges[1000], ('t', 'ro'))\n",
    "assert_equal(merges[-1], ('des', 'cendants_'))\n",
    "\n",
    "assert_equal(segmented_train[580], 'V ill age')\n",
    "assert_equal(segmented_train[5400], 'laun ching')\n",
    "\n",
    "assert_equal(segmented_test[501], 'C a the dr al')\n",
    "assert_equal(segmented_test[517], 'ho t els')\n",
    "\n",
    "assert(len(segmented_train_as_subwords) == 64166)\n",
    "assert(len(segmented_test_as_subwords) == 72712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28edb4c996cd5370b74da0f2cf2c2d94",
     "grade": false,
     "grade_id": "cell-e5d749c71ee045a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of subword types in the vocabulary of the training corpus:  4281\n",
      "The number of subword types in the vocabulary of the test corpus:  3595\n",
      "The percentage of test corpus vocabulary is OOV:  3.6995827538247568\n"
     ]
    }
   ],
   "source": [
    "# Assign your answers to the variables below:\n",
    "#     len_of_train_sub_vocab : int\n",
    "#     len_of_test_sub_vocab : int\n",
    "#     oov_sub_percentage : float (between between 0 and 100)\n",
    "# As in:\n",
    "# len_of_train_sub_vocab = <an expression that evaluates to the correct value>\n",
    "\n",
    "train_sub_vocab = set(segmented_train_as_subwords)\n",
    "test_sub_vocab = set(segmented_test_as_subwords)\n",
    "\n",
    "train_length = len(train_sub_vocab)\n",
    "test_length = len(test_sub_vocab)\n",
    "\n",
    "oov_subwords = test_sub_vocab - train_sub_vocab\n",
    "oov_sub_percentage = (len(oov_subwords) / len(test_sub_vocab)) * 100\n",
    "\n",
    "print(\"The number of subword types in the vocabulary of the training corpus: \", train_length)\n",
    "print(\"The number of subword types in the vocabulary of the test corpus: \", test_length)\n",
    "print(\"The percentage of test corpus vocabulary is OOV: \", oov_sub_percentage)\n",
    "\n",
    "# 1. The number of subword types in the vocabulary of the training corpus.\n",
    "len_of_train_sub_vocab = 4281\n",
    "\n",
    "# 2. The number of subword types in the vocabulary of the test corpus.\n",
    "len_of_test_sub_vocab = 3595\n",
    "\n",
    "# 3. What part of test corpus vocabulary is OOV?\n",
    "oov_sub_percentage  = 3.6995827538247568\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52a7f283612b83d9e035948729120df2",
     "grade": true,
     "grade_id": "cell-c29d08ffc3d29073",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(len_of_train_sub_vocab), int)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(500 < len_of_train_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f89238b288224d23ff386778f833f280",
     "grade": true,
     "grade_id": "cell-786b076f34bd1f3f",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(len_of_test_sub_vocab), int)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(500 < len_of_test_sub_vocab < 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f06200c47508df0805478460ec0aaac4",
     "grade": true,
     "grade_id": "cell-3d0f1cd7e311c960",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(oov_sub_percentage), float)\n",
    "# checks whether your answer is in the right ballpark\n",
    "assert(1 < oov_sub_percentage < 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "460afa1c221bd7ed34bbbc6111f66e0a",
     "grade": false,
     "grade_id": "cell-e9c3e0cf6893442b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3 <a class=\"anchor\" id=\"subtask_2_3\"></a>\n",
    "### Does the segmentation make sense? (1 Point)\n",
    "Now let's look a bit closer at the subwords that we've learned. Take a second to think if the results are what you would expect them to be.\n",
    "\n",
    "1. What are the top 10 most frequent subwords in the segmented test corpus? (in descending frequency order)\n",
    "2. What are the top 5 longest subwords in the segmented test corpus? (sort alphabetically if length is equal)\n",
    "3. What are the top 5 most frequent lengths of subwords in the vocabulary? (= among the unique subword types, not occurrences in corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17c3cd4850974eff70953b92dbb20139",
     "grade": false,
     "grade_id": "cell-e30eb3fbd26f4157",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most frequent subwords\n",
      "['the', ',', '.', 'of', 'and', 'a', 'in', 'to', 'on', 'is']\n",
      "top_5_by_len\n",
      "['representative', 'transportation', 'International', 'Revolutionary', 'approximately']\n",
      "top_5_freqs_of_lens\n",
      "[3, 4, 5, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "# Assign your answers to the variables below:\n",
    "#     top_10_by_freq : list of strings\n",
    "#     top_5_by_len : list of strings\n",
    "#     top_5_freqs_of_lens : list of integers\n",
    "# As in:\n",
    "# top_10_by_freq = <an expression that evaluates to the correct list>\n",
    "\n",
    "from collections import Counter\n",
    "subword_freq = Counter(segmented_test_as_subwords)\n",
    "\n",
    "# 1. Top 10 most frequent subwords\n",
    "top_10_by_freq = [word for word, freq in subword_freq.most_common(10)]\n",
    "\n",
    "# 2. Top 5 longest subwords in the segmented test corpus\n",
    "sorted_subwords_by_len = sorted(set(segmented_test_as_subwords), key=lambda x: (-len(x), x))\n",
    "top_5_by_len = sorted_subwords_by_len[:5]\n",
    "\n",
    "# 3. Calculate the frequencies of lengths of subwords in the vocabulary\n",
    "lengths_freq = Counter(len(word) for word in set(segmented_test_as_subwords))\n",
    "top_5_freqs_of_lens = [length for length, freq in lengths_freq.most_common(5)]\n",
    "\n",
    "print(\"Top 10 most frequent subwords\")\n",
    "print(top_10_by_freq)\n",
    "print(\"top_5_by_len\")\n",
    "print(top_5_by_len)\n",
    "print(\"top_5_freqs_of_lens\")\n",
    "print(top_5_freqs_of_lens)\n",
    "\n",
    "\n",
    "# 1. What are the top 10 most frequent subwords in the segmented test corpus?\n",
    "top_10_by_freq = ['the', ',', '.', 'of', 'and', 'a', 'in', 'to', 'on', 'is']\n",
    "\n",
    "# 2. What are the top 5 longest subwords in the segmented test corpus?\n",
    "top_5_by_len = ['representative', 'transportation', 'International', 'Revolutionary', 'approximately']\n",
    "\n",
    "# 3. What are the top 5 most frequent lengths of subwords in the vocabulary?\n",
    "top_5_freqs_of_lens = [3, 4, 5, 6, 2]\n",
    "\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4217d95d9047603698f43c5cb5e8a321",
     "grade": true,
     "grade_id": "cell-e7786799f7aeacdc",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 1 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(top_10_by_freq), list)\n",
    "assert_equal(type(top_10_by_freq[0]), str)\n",
    "assert_equal(len(top_10_by_freq), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7d60bc18da4a01ee65f2057486bd4eb",
     "grade": true,
     "grade_id": "cell-7f4032e3c0f62c07",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 2 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(top_5_by_len), list)\n",
    "assert_equal(type(top_5_by_len[0]), str)\n",
    "assert_equal(len(top_5_by_len), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f4c6298101b9b0b7da5a0f22aece981",
     "grade": true,
     "grade_id": "cell-68ac266fbfcf1cef",
     "locked": true,
     "points": 0.4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUESTION 3 tests\n",
    "\n",
    "# checks whether your answer is of the right data type\n",
    "assert_equal(type(top_5_freqs_of_lens), list)\n",
    "assert_equal(type(top_5_freqs_of_lens[0]), int)\n",
    "assert_equal(len(top_5_freqs_of_lens), 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0bb60d97240b4ebbb3a50923a76a418",
     "grade": false,
     "grade_id": "cell-f67771f63a0e8969",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.4 <a class=\"anchor\" id=\"subtask_2_4\"></a>\n",
    "### Your thoughts (3 Points)\n",
    "Briefly answer the following questions:\n",
    "\n",
    "1. Describe what happens when you change the k parameter? How could one find a find a good value for k?\n",
    "\n",
    "2. What are the possible NLP applications that can benefit from the tokenization into subwords? Why?\n",
    "\n",
    "3. How could the OOV number for our data be lowered further without changing anything in the segmentation procedure (k stays the same)? Hint: think about the things we did in the previous assignments but are not doing in this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "819985af5d5b7176cd09585c696d6ec9",
     "grade": true,
     "grade_id": "cell-18c675e80eea69ed",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**1. Describe what happens when you change the k parameter? How could one find a find a good value for k?**\n",
    "\n",
    "The number of BPE merges is determined by the k parameter, which has an impact on the subwords set. Longer subwords would arise from k values, whereas shorter subwords would result from lower k values. A good value of k can be the square root of the training corpus's size, then we adjust k gradually based on some metrics such as minimizing OOV rates\n",
    "\n",
    "**2. What are the possible NLP applications that can benefit from the tokenization into subwords? Why?**\n",
    "\n",
    "Subword tokenization helps many NLP applications such as text classification, machine translation and named entity recognition (NER). For machine translation, subword tokenization can help translate unseen words by using morphological similarities. For text classification, subword tokenization helps recognize semantic similarities between words sharing common subwords. Finally, NER tasks is classifying named entities (such as person names, organizations, locations, etc.) into predefined categories. Subword tokenization can improve NER models by providing more representations of entity names.  \n",
    "\n",
    "**3. How could the OOV number for our data be lowered further without changing anything in the segmentation procedure (k stays the same)? Hint: think about the things we did in the previous assignments but are not doing in this one.**\n",
    "\n",
    "Lowering the OOV rate further, while keeping the parameter k constant, can be achieved through preprocessing methods such as normalizing text by converting it to lowercase, applying stemming, or lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6e88584737c8d18fd98a62c5fb3a8e6",
     "grade": false,
     "grade_id": "cell-af66e3f2833de7d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checklist before submission <a class=\"anchor\" id=\"checklist\"></a>\n",
    "### 1\n",
    "To make sure that you didn't forget to import some package or to name some variable, press **Kernel -> Restart** and then **Cell -> Run All**. This way your code will be run exactly in the same order as during the autograding.\n",
    "### 2\n",
    "Click the **Validate** button in the upper menu to check that you haven't missed anything. You might need to run the validation in the terminal because BPE algorithm takes time.\n",
    "### 3\n",
    "To submit the notebook, click on the **jupyterhub** logo in the upper left part of the window, choose the **Assignments** folder, and press **submit**. You can submit multiple times, only the last one counts.\n",
    "### 4\n",
    "Please fill in the feedback form in the [Assignment](https://mycourses.aalto.fi/mod/questionnaire/view.php?id=1122382) section of Mycoures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
